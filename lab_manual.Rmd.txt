--- 
title: "BASV 316 R Lab Manual"
author: "George Self"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is the lab manual for the University of Arizona South BASV 316 class."
github-repo: "grself/BASV316_Lab_Manual"
---

<!--

TODO:

180211: None at this time

-->

# Preface {-}

I have taught BASV 316, _Introductory Methods of Analysis_, online for the University of Arizona in Sierra Vista since 2010 and enjoy working with students on research methods. From the start, I wanted students to work with statistics as part of our studies and carry out the types of calculations that are discussed in the text. As I evaluated statistical software I had three criteria:

* **Open Educational Resource (OER)**. It is important to me that students use software that is available free of charge and is supported by the entire web community.

* **Platform**. While most of my students use a Windows-based system, some use Macintosh and it was important to me to use software that is available for all of those platforms. As a bonus, most OER software is also available for the Linux system, though I am not aware of any of my students who are using Linux. Finally, I occasionally have students who are not able to load software on their personal computers (think: *Chromebook*) so I needed an online capability.

* **Longevity**. I wanted a system that could be used in other college classes or in a business setting after graduation. That way, any time a student spends learning the software in my class will be an investment that can yield results for many years.

*R* (just a single letter, *R*) met those objectives and that is the software I chose to use. This manual started as a series of six lab exercises using *R* but has grown over the years to the ten topics covered in this edition. Moreover, *R* is a recognized standard for statistical analysis and could be easily used for even peer-reviewed published papers. It is my hope that students will find the labs instructive and they will then be able to use *R* for other classes. 

This lab manual is written with *Bookdown* tools in *RStudio*. It is published under a *Creative Commons 0 Universal* license, essentially "public domain," (see [Creative Commons License]) with a goal that other instructors can modify and use it to meet their own needs. The source can be found at [GITHUB](https://goo.gl/6dZqCK) and I always welcome comments. Finally, it was written with base R (R Core Team (2017). R: A language and environment for statistical computing. [R Foundation for Statistical Computing, Vienna, Austria.](https://www.R-project.org/)).

--George Self


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# (PART) Introduction {-}

# Introduction to R {#lab1}

```{r setup, include=FALSE}
# Load some libraries
library(here)

knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%",
  message=FALSE,
  warnings=FALSE
)

# Set the rproj home directory
here::here()

# Turn on the DataCamp Light interactive tools
tutorial::go_interactive()

```

Statistical analysis is the core for nearly all research projects and researchers have a wide variety of statistical tools that they can use, like _SPSS_ and _SAS_. Unfortunately, these analysis tools are expensive or difficult to master so this lab manual introduces _R_, a powerful, open source statistical analysis program that is available free of charge. But before diving into a statistics package there is one important background fundamental that must be covered: data.

-----

## Types of Data

There are two main types of data and it is important to understand the difference between them since that determines appropriate analytical tests.

* Continuous data are integer or decimal numbers and are typically used for counts or measures -- like a person's weight, a tree's height, or a car's speed. Continuous data are measured with scales that have equal divisions so the difference between any two values can be calculated. Because continuous data include characteristics like means and standard deviations, they are analyzed using parametric tests.

* Categorical data group observations into a limited number of categories; for example, type of pet (cat, dog, bird, etc.) or place of residence (Arizona, California, etc.). One common type of categorical data is generated with an "agree-disagree" type of scale, like "I enjoy reading: Strongly Agree : Agree : Neutral : Disagree : Strongly Disagree." Because categorical data do not have characteristics like means or standard deviations, they are analyzed using nonparametric tests.

-----

## The R Command Line

All _R_ commands are entered from a "Command Line" environment. Many students find this a bit challenging at first but once they learn some foundational concepts the command line becomes easy and fast to use. This is an explanation for the _R_ script in the box below.

### Demonstration: The R Command Line

* Line 1: This is a comment that is used to record notes in a script. In _R_, all comments start with a hash-mark (#) and everything after that symbol is ignored. Comments are used frequently in scripts presented in this manual in order to explain what the script is doing. Good programmers comment liberally so team members can easily figure out what they did.

* Line 2: Calculate the value of 3+5.

* Line 3: Calculate the value of "5 + 8 * 2".

* Lines 6-7: These lines create two variables, `MaxScore` and `MinScore`, and then assign values to the variables. You should note two important things about these lines. First, the "assignment" operator is a less than sign followed by a hyphen, making a leftpointing arrow like `<-`. That tells _R_ to store the number on the right side of the arrow operator into the variable named on the left side of the line. Also, keep in mind that capitalization matters with _R_. Thus, the variable named `MaxScore` would be different than a variable named `maxscore`. These lines only store values in variables and nothing gets printed to the screen.

<div class="grsnote">A _variable_ is nothing more than a place in memory to store temporary data. Think of it as a "box" that is used to store something until it is needed later.</div>

* Line 8: The variable `Range` is filled with the result of subtracting `MaxScore` minus `MinScore`.

* Line 9: Entering a variable name, like `Range`, on a line by itself causes the value stored in that variable to be displayed.

* Line 12: In _R_, a list of numbers can be stored in a single variable by using the "combine" function, which is a _c_ followed by a list of the numbers inside a parenthesis. This line creates a variable called `TestScores` and then stores a list of six numbers in that variable.

* Line 13: The contents of the variable `TestScores` is printed to the screen.

```{r}
# Some basic calculations
3 + 5
5 + 8 * 2

# Using variables
MaxScore <- 57
MinScore <- 22
Range <- MaxScore - MinScore
Range

# The combination function
TestScores <- c(85, 83, 92, 88, 71, 85)
TestScores
```

<div class="grsnote">The _DataCamp_ blocks found throughout this lab manual are designed to "try out" _R_ commands. Click the yellow "Run" button in the lower left corner to execute the entire script and see the results. There are two panels available in the box. Clicking _script.R_ in the top of the box displays the _R_ script and clicking _R Console_ displays the results of executing the script. The commands in the script can be modified and rerun in order to "play around" with _R_. Also, _R_ commands can be entered directly in the *R Console* and executed by tapping the `ENTER` key.</div>

### Skill Check: The R Command Line

Now is a time to try some of the command line skills demonstrated above. In the following _R_ codebox, calculate these values. 

Notes:

1. In the second line the `^` symbol means "raise to the power of" so that line reads "24 plus the value of 2 raised to the 6th power."

2. In the third line the `sqrt()` function calculates the square root of the number in the parenthesis, or the square root of 9 in this example.

* Set the variable M equal to 15 + ( 37 * 2 )

* Set the variable N equal to 24 + ( 2 ^ 6 )

* Set the variable P equal to 15 - ( sqrt(9) )


```{r ex="ex01", type="pre-exercise-code"}

# No pre-exercise code for this exercise

```

```{r ex="ex01", type="sample-code"}

# Set M to 15 + ( 37 * 2 )


# Set N to 24 + ( 2 ^ 6 )


# Set P to 15 - ( sqrt(9) )


```


```{r ex="ex01", type="solution"}

M <- 15 + ( 37 * 2 )

N <- 24 + ( 2 ^ 6 )

P <- 15 - ( sqrt(9) )

```

```{r ex="ex01", type="sct"}

ex() %>% check_object("M") %>% check_equal()
ex() %>% check_object("N") %>% check_equal()
ex() %>% check_object("P") %>% check_equal()

success_msg("Good Work! Now you can use R to calculate both simple and complex math problems!")

```

-----

## Data Frames

A _data frame_ is a collection of data generated during a research project. An example data frame that is easy to understand would be a spreadsheet that contains the times recorded for a race. _R_ comes configured with a 103 built-in data frames used for training and the _R_ script below is an introduction to one of the data frames used in several of the labs in this manual: _mtcars_.

### Demonstration: Data Frames

* Line 2: Entering the name of the data frame, [_mtcars_](/tutorials/9999-data-dict/index.html#mtcars), on a line by itself causes _R_ to print the contents of the entire the data frame to the screen. Since _mtcars_ is rather small it is fine to print it to the screen, but some data frames have hundreds of lines and that may cause the screen to "scroll" for some time before the end of the data frame is reached.

* Line 3: This prints the structure of the _mtcars_ data frame. The result shows that this is a _data.frame_ and has 32 observations (that is how many cars are in the dataset) of 11 variables (things like _mpg_). Also the structure command displays the type of data that are in the dataset. For example, all 11 variables are of the "number" type. The `str` function is frequently used to better understand a data frame.

* Line 4: This line prints the maximum _mpg_ value for the _mtcars_ data frame. Note that the specific variable desired is indicated by both the data frame name and the variable, separated by a dollar sign, like _mtcars$mpg_ on this line.

* Line 5: This prints the minimum _mpg_ value.

*If some of the lines in the result are too long to fit on one row in the R Console they will wrap around.*


```{r}
# The Motor Trend Cars Data Frame
mtcars
str(mtcars)
max(mtcars$mpg)
min(mpg)
```

## Skill Check: Data Frames

In the following _R_ codebox, explore the [_airquality_](/tutorials/9999-data-dict/index.html#airquality) data frame. 

* Determine the structure of the _airquality_ data frame

* Set MaxWind to the maximum value of _Wind_

* Set MaxTemp to the maximum value of _Temp_

* Set MinOzone to the minimum value of _Ozone_

```{r ex="ex02", type="pre-exercise-code"}

# No pre-exercise code for this exercise

```

```{r ex="ex02", type="sample-code"}

# Determine the structure of the airquality data frame


# Set max.wind to the maximum value of Wind


# Set max.temp to the maximum value of Temp


# Set min.ozone to the minimum value of Ozone


```


```{r ex="ex02", type="solution"}

str(airquality)

MaxWind <- max(airquality$Wind)

MaxTemp <- max(airquality$Temp)

MinOzone <- min(airquality$Ozone)

```

```{r ex="ex02", type="sct"}

ex() %>% check_object("MaxWind") %>% check_equal()
ex() %>% check_object("MaxTemp") %>% check_equal()
ex() %>% check_object("MinOzone") %>% check_equal()

success_msg("Perfect! Thanks for exploring this built-in data frame!")

```

## Next

This tutorial was an introduction to _R_ and how to complete basic arithmetic calculations. The next tutorial explores several different commonly-used [central measures](/tutorials/0020-central-measures/) and how to calculate those measures.

<!--chapter:end:01-lab01-intro.Rmd-->

# (PART) Descriptive Statistics {-}

# Central Measures {#lab2}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%",
  message=FALSE,
  warnings=FALSE
)
```


```{r, include=FALSE}
library(tutorial)
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```

## Concepts

It is often desirable to characterize an entire vector of numbers with a single value and the number that is "in the middle" of the vector would seem most logical to use. Students in elementary school are taught how to find the average of a group of numbers and they learn that the average is the best representation for that entire group. In statistics, though, there are several different numbers that are often used to represent an entire vector of numbers and these are  collectively known as the Central Measures, or numbers that are the "middle" of the vector.

<div class="grsnote">
A *vector* is nothing more than a list of related items. For example, a vector of street names may look like c("elm", "main", "first", "raven") and a vector of test scores may look like c(83, 91, 76, 88). Notice that vectors in R are created with a "c" (for "combine") operator.
</div>

### N

One of the simplest of measures is nothing more than the number of items in a vector. For example, for the vector 5, 7, 13, 22 the *N* (number of items) is 4. Technically, N does not identify the middle of a vector but it is an important measure and is included here for completeness.

#### Finding N

To find the number of items in a vector (N), use the *length* procedure as found in the R script below.

* Line 2: The length of the *rivers* vector is printed, which is the same as the number of items in that vector, or N.
* Line 3: The length of the *faithful\$eruptions* vector is printed, which is the same as the number of items in the vector, or N. Notice that because the *faithful* data frame contains more than one vector it is necessary to specify both the data frame name and the vector name with a dollar sign between them.

```{r}
# Finding N
length(rivers)
length(faithful$eruptions)
```


### Mean

The mean is calculated by adding all of the data items together and then dividing that sum by the number of items (N), which is taught in elementary school as the average. For example, given the vector: 6, 8, 9, the total is 23 and that divided by 3 (N) is 7.66; so the mean of 6, 8, 9 is 7.66. 

#### Finding the Mean

To find the mean of a vector, use the "mean" procedure as found in the R script below.

* Line 2: Attach the *faithful* data frame for the next two lines.
* Lines 3-4: Calculate the means for *eruptions* and *waiting*.
* Line 5: Detach the *faithful* data frame.
* Line 8: Attach the *iris* data frame for the next four lines.
* Lines 9-12: Calculate the means for various variables in the *iris* data frame.
* Line 13: Detach the *iris* data frame.


```{r}
# Find means for faithful
attach(faithful)
mean(eruptions)
mean(waiting)
detach(faithful)

# Find means for iris
attach(iris)
mean(Sepal.Width)
mean(Sepal.Length)
mean(Petal.Width)
mean(Petal.Length)
detach(iris)
```

#### Finding the Trimmed Mean

If a vector has outliers, or values that are unusually large or small, then the mean is often skewed such that it no longer represents the "average" value. As an example, the length (in miles) of the 141 longest rivers in North America ranges from 135 to 3710 and the mean of these values is 591 miles (these data are found in the *rivers* data frame included with R). Unfortunately, because the lengths of the top few rivers are disproportionately higher than the rest of the values in the vector (their lengths are outliers), the mean is skewed upward. One way to compensate for outliers is to use a trimmed mean (sometimes called a truncated mean). A trimmed mean is calculated by removing a specific number of values from both the top and bottom of the vector and then finding the mean of the remaining values. In the case of the rivers vector, if 5% of the values are removed from both the top and bottom (7 values from each end of the vector, for 10% total) then 127 values remain with a range from 230 to 1450 and the trimmed mean for that vector is 519. It is not possible, or desirable, to trim a different amount from the top and bottom. A trimmed mean always trims the same amount from both ends of the vector. Trimming the vector effectively removes both upper and lower outliers and produces a much more reasonable central value for this vector. In actual practice, a trimmed mean is not commonly used since it is difficult to know how much to trim from the vector and the resulting mean may be just as skewed as if no values were trimmed; thus, when outliers are suspected, the best "middle" term to report is the median.

<div class="grsnote">
Outliers are more thoroughly defined and mathematically calculated in [Lab 3](#lab3outliers).
</div>


The trimmed mean can be easily found in R by adding a "single"trim"" parameter to the *mean* command, as demonstrated in the following script. 

* Lines 2-4: calculate the trimmed mean by including "trim=0.15" in the *mean* function. Line 2 trims 15% from each end of the *rivers* vector and Line 3 trims 25%. 

```{r}
# Find trimmed means for rivers
mean(rivers)
mean(rivers, trim=0.15)
mean(rivers, trim=0.25)

# Find trimmed means for attenu
attach(attenu)
mean(accel)
mean(accel, trim=0.15)
mean(accel, trim=0.25)
detach(attenu)
```

### Median

The median is found by listing all of the data items in numeric order and then mechanically finding the middle item. For  example, using the vector 6, 8, 9, the middle item (or median) is 8. If the vector has an even number of items, then the median is calculated as the mean between the two middle items. For example, in the vector 6, 8, 9, 13 the median is 8.5, which is the mean of 8 and 9, the two middle terms. 

The median is very useful in cases where the vector has outliers. As an example of using a median rather than a mean, consider the vector 5, 6, 7, 8, 30. The mean is (5+6+7+8+30)/5 = 56/5 = 11.2. However, 11.2 is clearly much higher than most of the other numbers in that vector since one outlier, 30, is significantly skewing the mean upward. A much better representation of the center of this vector is 7, which is the median. To re-visit the river lengths introduced above, the median of the vector is 425, which is much more representative of the "middle" length than using either the mean or the trimmed mean.

As another example where the median is the best central measure, suppose a newspaper reporter wanted to find the "average" wage for a group of factory workers. The ten workers in that factory all have an annual salary of \$25,000; however, the supervisor has a salary of \$125,000. In the newspaper article, the supervisor is quoted as saying that his workers have an average salary of \$34,090. That is correct if the mean of all those salaries (including the supervisor's) is reported, but that number is clearly higher than any sort of reasonable "average" salary for workers in the factory due to the one outlier. In this case, the median of \$25,000 would be much more representative of the "average" salary. The median is typically reported for salaries, home values, and other vectors where one or two outliers typically distort the reported "middle" value.

If the vector contains no outliers, then the mean and median are the same; but if there are outliers then these two measures become separated, often by a large amount. Consider the *rivers* vector mentioned in the Mean section above. That vector has a mean of 591 and a median of 425. This difference, 166, is about 28% of the mean and is significant. The size of this difference would tell a researcher that there are outliers in the vector that may be skewing the mean.

#### Finding the Median

To find the median of a vector, use the "median" procedure as found in the R script below.

* Line 2: Attach the *faithful* data frame for the next two lines.
* Lines 3-4: Calculate the medians for *eruptions* and *waiting*.
* Line 5: Detach the *faithful* data frame.
* Line 8: Attach the *iris* data frame for the next four lines.
* Lines 9-12: Calculate the medians for various variables in the *iris* data frame.
* Line 13: Detach the *iris* data frame.

```{r}
# Find medians for faithful
attach(faithful)
median(eruptions)
median(waiting)
detach(faithful)

# Find medians for iris
attach(iris)
median(Sepal.Width)
median(Sepal.Length)
median(Petal.Width)
median(Petal.Length)
detach(iris)

```


### Mode

The mode is used to describe the center of nominal or ordinal data and is nothing more than the item that was most commonly found in the vector. For example, if a question asked respondents to select their zip code from a list of five local codes and "12345" was selected more often than any other then that would be the mode for that item. Calculating the mode is no more difficult than counting the number of times the various values are found and choosing the one that is most frequent. As an example, the *mtcars* data frame lists the following for the number of cylinders in each car:

```{r cyltable, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', tut=FALSE}

dt <- as.data.frame(table(mtcars$cyl))
names(dt) <- c("Cylinders","N")

kable(dt, "html") %>%
  kable_styling(bootstrap_options = "striped", 
                full_width = F,
                position = "center"
                )

```

Since 14 cars have 8 cylinders and that is the most common number of cylinders, that would be the mode for this data item.

It does not make much sense to calculate the mean or median for categorical data since those have no numeric value; however, reports frequently list the mean for Likert-style questions (ordered categorical data) by equating each level of response to a number and then calculating the mean of those numbers. For example, imagine that a student housing survey asked respondents to select among "Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree" for a statement like "I like the food in the cafeteria." That is clearly ordered categorical data and while "Agree" is somehow better than "Disagree" it would be wrong to try to quantify that difference as "two points better." Sometimes, though, researchers will assign a point value to those responses like "Strongly Disagree" is one point, "Disagree" is two points, and so forth. Then they will calculate the mean for the responses on a survey item and report something like "The question about the food in the cafeteria had a mean of 3.24." It would be impossible to know how to interpret that sort of number. Are students 0.24 units above "Neutral" on liking the cafeteria food?

There is no procedure for finding mode in this lab, but [Lab 5](#lab5) discusses frequency tables and by creating a frequency table it is easy to determine the mode of a variable. In fact, the *mtcars\$cylinders* table above was created with a *table* command in R and that was used to determine the mode for *cylinders*.

## Activities

### Activity 1: Central Measures

Using the *cafe* data frame, find the mean and median for both *age* and *bill*. Include the answers to these problems in the deliverable document for this lab.

```{r ex="act2.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act2.1", type="sample-code"}
# Using the cafe data frame, find the mean and median for both age and bill.


```

### Activity 2: Trimmed Mean

Using the *cafe* data frame, find the mean for *bill* with 5% trimmed from the data. Include that value in the deliverable document for this lab.

```{r ex="act2.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act2.2", type="sample-code"}
# Using the cafe data frame, find the mean for bill with 5% trimmed from the data.


```

## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 2," like "George Self Lab 2" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:02-lab02.Rmd-->

# Dispersion {#lab3}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%",
  message=FALSE,
  warnings=FALSE
)
```

```{r, include=FALSE}
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```


One way to describe a vector is to report its dispersion, or spread. For example, if a poll of 100 potential Chevrolet customers were asked which model they preferred, it would make a lot of difference to a local automobile seller if the responses were fairly evenly distributed among Impala, Camaro, Traverse, and Corvette compared to results where customers selected 90% Impala with the rest distributed among the others. How tightly survey results are grouped (or scattered) is called "dispersion" and this lab explores data dispersion and the methods used to calculate that value with R.

## Measures of Data Dispersion

### Range

The maximum and minimum values are those at the extreme ends of a vector and the range is nothing more than the maximum minus the minimum values. For the 2016 version of the Scholastic Aptitude Test (SAT) the maximum score is 1600 and the minimum score is 400, so the range is 1600 - 400. Occasionally, the range is expressed as a single number, 1200 in the case of the SAT scores, but researchers normally want to know the actual endpoints rather than just the spread and those endpoints are what R reports for range.

To find the range of a vector, use the `range` function as found in the R script below.

* Line 2: Attach the trees data frame to make it easier to enter the range functions.
* Lines 3-5: These are cat functions that display the ranges for three different variables in the *trees* data frame.
* Line 6: This detaches the *trees* data frame since no further functions will be accessing it.
* Lines 8-12: These lines are very similar to Lines 1-6 but use variables in the *rock* data frame.

```{r}
# Range for trees
attach(trees)
range(Girth)
range(Height)
range(Volume)
detach(trees)

# Range for rock.
attach(rock)
range(area)
range(perm)
detach(rock)
```

### Quartiles

A measure that is closely related to the median is the first and third quartile. The first quartile (Q1) is the score that splits the lowest 25% of the values from the rest and the third quartile (Q3) splits the highest 25% of the values from the rest. The second quartile (Q2) is the same as the median and, normally, the term "median" is used rather than Q2. For example, consider this vector: 5, 7, 10, 13, 17, 19, 23. The median of this vector is 13 because three values are smaller and three are larger. The first quartile is 7, which is the median for the lower half of the values (not including 13, the median of the entire vector); or the score that splits the lowest 25 % from the rest of the data. The third quartile is 19, which is the median for the upper half of the scores; or the score that splits the highest 25 % from the rest of the data.

Occasionally, the word "hinges" appears in statistical literature. The two hinges for a vector are the medians for the lower half and the upper half of the data, but those halves also include the median for the entire vector. For the simple vector above, the lower hinge is the median of 5, 7, 10, and 13, or 8.5. The upper hinge is the median of 13, 17, 19, and 23, or 18. Quartiles and hinges usually have about the same accuracy but quartiles are more commonly used.

R calculates and prints the quartiles, along with several other descriptive measures, using the `summary()` function. This is a very important function that returns a great deal of useful information and is a staple in a data scientest’s toolbox. To find the quartiles of a vector, use the `summary` function found in the R script below.

* Line 2: Attach the *trees* data frame to make it easier to enter the `summary` functions.
* Lines 3-5: Display summary information for three different variables in the *trees* data frame.
* Line 6: Detach the *trees* data frame.
* Lines 8-14: Display summary information for four different variables in the *rock* data frame.


```{r}
# Summary for trees
attach(trees)
summary(Girth)
summary(Height)
summary(Volume)
detach(trees)

# Summary for rock.
attach(rock)
summary(area)
summary(peri)
summary(shape)
summary(perm)
detach(rock)
```


### IQR (Inter-Quartile Range)

Another measure of dispersion that is occasionally used is the Inter-Quartile Range (IQR); that is, the difference between Q1 and Q3. To The IQR function is find the IQR for a vector, use the IQR command: IQR(hp). R reports that the difference between Q1 and Q3 for the horsepower vector is 83.5. Since the IQR removes outliers it may sometimes provide a better indication of the data dispersion. To find the inter-quartile range of a vector, use the `IQR` function found in the R script below. *Note: `IQR` is one of the few functions in R that uses upper-case letters.*

* Line 2: Attach the *trees* data frame to make it easier to enter the `IQR` functions.
* Lines 3-5: Display inter-quartile range for three different variables in the *trees* data frame.
* Line 6: Detach the *trees* data frame.
* Lines 8-14: Display inter-quartile range for four different variables in the *rock* data frame.


```{r}
# Summary for trees
attach(trees)
IQR(Girth)
IQR(Height)
IQR(Volume)
detach(trees)

# Summary for rock.
attach(rock)
IQR(area)
IQR(peri)
IQR(shape)
IQR(perm)
detach(rock)
```


### Outliers {#lab3outliers}

It is useful to consider data observations that are far outside the "normal" in any given vector. For example, imagine a neighborhood where the houses all cost about \$150,000. Suppose someone wins a lottery and decides to build a \$500,000 house in that same neighborhood. The value of that house would be an outlier in a vector that contains the house values in the neighborhood; that is, it would be outside the "average" house value. Outliers are important when discussing central measure since they tend to skew certain types of measures.

Statistically, outliers are defined as values that lie outside boundaries that are 1.5 times the Inter-Quartile Range (IQR) below the first quartile or above the third quartile. As an example, consider the *trees$Volume* vector.

```{r sumTrees, echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
cat("Summary:\n")
summary(trees$Volume)
cat("\nIQR: ", IQR(trees$Volume))

```

The first quartile is 19.40 and the IQR is 17.9. To calculate the lower boundary for outliers subtract Q1 - (1.5*IQR), which equals -7.45. Since it is not possible to have a negative tree volume, the lower boundary is assumed to be 0. 

The third quartile is 37.30 and the IQR is 17.9. To calculate the upper boundary for outliers add Q3 + (1.5*IQR), which equals 64.15. 

To determine the values that lie outside the boundaries, use the R 'sort' function to list all values in numeric order:

```{r sortTrees, echo=TRUE, message=FALSE, warnings=FALSE, tut=FALSE}
sort(trees$Volume)
```

No values are less than than the lower boundary of 0 and only one value, 77, is greater than the upper boundary of 64.15 so that would be the only outlier.

The following script creates an R function to calculate the lower and upper boundaries.

* Line 2: Attach the *trees* data frame for this exercise.
* line 6: In statistics, quantiles are cutoff points that divide a vector into parts. One of the most commonly-used divisions is to split the vector into four parts, commonly called "quartiles" (as in the summary function). Line 6 is used to find the cutoff point for the 25th percentile and that value is then stored in a variable named Q1 (for "Quartile 1").
* Line 8: The value of 1.5 times the IQR for *Volume* is calculated and stored in a variable named *Fac* (for "Factor").
* Line 10: The lower boundary is calculated by subtracting *Fac* from Q1 and storing the result in *LowerBoundary*.
* Lines 12-16: These lines determine the upper boundary using the same technique as for the lower boundary.
* Lines 19-20: These lines print the calculated outlier boundaries.
* Line 21: A `print()` function is used since it creates a better formatted screen display for the long *Volume* listing. Also note that the vector is sorted so outlier values would be easier to spot at the start and end of the list of values.


```{r}
# Calculate Outlier Boundaries
attach(trees)

# Calculate the low boundary
# Find the first quartile
Q1 <- quantile(Volume,0.25)
# Find 1.5 X IQR
Fac <- (1.5 * IQR(Volume))
# Calc lower boundary
LowerBoundary <- (Q1 - Fac)

# Calculate the high boundary
# Find the third quartile
Q3 <- quantile(Volume,0.75)
# Calc high boundary
UpperBoundary <- (Q3 + Fac)

# Look at the sorted vector
print(LowerBoundary)
print(UpperBoundary)
print(sort(Volume))

detach(trees)
```

*Note: [Lab 4](#lab4) demonstrates a graphical method to determine if a vector contains outliers. Using the graphical method will make it simple to determine if a vector has outliers and then the above script can identify those outlying values.*

### Standard Deviation

The standard deviation of a vector is a number that indicates how much variation there are in the data; or how "scattered" the data are from the mean. In general, the larger the standard deviation then the more variation there is in the data. A vector with a small standard deviation would create a sharply peaked normal distribution curve while a large standard deviation would create a flatter curve. 

Once a standard deviation is calculated, then about 68.2% of the samples will lie closer to the mean than that number. To put it another way, one standard deviation explains about 68.2% of the variance from the mean. To show this concept graphically, consider the three following graphs of IQ scores.

```{r stdevPlot, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', tut=FALSE}
# Display three different plots that illustrate the first three standard deviations on IQ data
par(mfrow=c(2,2))

#############################
# One standard deviation
mn=100; std=15
lb=100-std; ub=100+std

x <- seq(-4,4,length=100)*std + mn
hx <- dnorm(x,mn,std)

plot(x, hx, 
     type="n", 
     xlab="IQ Values", 
     ylab="",
     main="1 St Dev", 
     axes=FALSE)

i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="red") 

area <- pnorm(ub, mn, std) - pnorm(lb, mn, std)
#result <- paste("P(",lb,"< IQ <",ub,") =",
#                signif(area, digits=3))
#mtext(result,3)
axis(1, at=seq(40, 170, 40), pos=0)

#############################
# Two standard deviations
mn=100; std=15
lb=100-(std*2); ub=100+(std*2)

x <- seq(-4,4,length=100)*std + mn
hx <- dnorm(x,mn,std)

plot(x, hx, type="n", 
     xlab="IQ Values", 
     ylab="",
     main="2 St Dev", 
     axes=FALSE)

i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="blue") 

area <- pnorm(ub, mn, std) - pnorm(lb, mn, std)
# result <- paste("P(",lb,"< IQ <",ub,") =",
#                 signif(area, digits=3))
# mtext(result,3)
axis(1, at=seq(40, 160, 40), pos=0)

#############################
# Three standard deviations
mn=100; std=15
lb=100-(std*3); ub=100+(std*3)

x <- seq(-4,4,length=100)*std + mn
hx <- dnorm(x,mn,std)

plot(x, hx, 
     type="n", 
     xlab="IQ Values", 
     ylab="",
     main="3 St Dev", 
     axes=FALSE)

i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="green") 

area <- pnorm(ub, mn, std) - pnorm(lb, mn, std)
# result <- paste("P(",lb,"< IQ <",ub,") =",
#                 signif(area, digits=3))
# mtext(result,3)
axis(1, at=seq(40, 160, 40), pos=0)

```


The mean of an IQ distribution is 100 and one standard deviation is 15. The shaded area under the first curve, in red, includes about 68.2% of all IQ scores. In the same way, two standard deviations from the mean would include about 95.4% of the data points and is illustrated in blue in the second image; and three standard deviations would include more than 99.7% of the data points and is illustrated in green in the third image.

As one last example, imagine several classes with a total of 500 students where the professors administered an examination worth 100 points. If the mean score for that examination was 80 and the standard deviation was 5, then the professors would know that the scores were fairly tightly grouped (341 scores of the 500 (68.2%) were between 75-85, within 5 points of the mean), and this would probably be good news. On the other hand, if the mean score was 60 and the standard deviation was 15, then the scores were "all over the place" (more precisely, 341 scores of the 500 were between 45-75), and that may mean that the professors would have to re-think how the lesson was taught or maybe that the examination itself was flawed.

It is difficult to categorically state whether a specific standard deviation is good or bad; it is simply a measure of how concentrated the data are around the mean. For something like a manufacturing process where the required tolerance for the parts being produced is tight then the standard deviation for the weights of random samples pulled off of the line must be very small; that is, the parts must be as nearly identical as possible. However, in another context, the standard deviation may be quite large. Imagine measuring the time it takes a group of high school students to run 100 yards. Some would be very fast but others would be much slower and the standard deviation for that data would likely be large.

To find the standard deviation of a vector, use the "sd" function as found in the R script below.

* Line 2: Attach the *trees* data frame to make it easier to enter the sd functions.
* Lines 3-5: Display standard deviation for three different variables in the *trees* data frame.
* Line 6: Detach the *trees* data frame.
* Lines 8-14: Display standard deviation for four different variables in the *rock* data frame.


```{r}
# Summary for trees
attach(trees)
sd(Girth)
sd(Height)
sd(Volume)
detach(trees)

# Summary for rock.
attach(rock)
sd(area)
sd(peri)
sd(shape)
sd(perm)
detach(rock)
```

In the results, notice that the tree girth has a standard deviation of about 3.14 inches while the range was 8.3-20.6 inches (the range function is found in Section 3.1.1). While that range is fairly large, the standard deviation indicates that the girths of most of the trees were within about 6 inches of each other, so the trees were all about the same size. This is an example of how the standard deviation can provide information to help interpret the range.

## Activities

### Activity 1: Range

Using the *cafe* data frame, find the range for both *age* and *bill*. Include the answers to these problems in the deliverable document for this lab.

```{r ex="act3.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act3.1", type="sample-code"}
# Using the cafe data frame, find the range for both age and bill.
```


### Activity 2: Quartiles Using Summary

Using the *cafe* data frame, find the summary for both *age* and *bill*. Include the values for Q1 and Q3 for both variables in the deliverable document for this lab.

```{r ex="act3.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act3.2", type="sample-code"}
# Using the cafe data frame, find the summary for bill.
```

### Activity 3: Inter-Quartile Range (IQR)

Using the *cafe* data frame, find the inter-quartile-range (IQR) for both *age* and *bill*. Include the answers to these problems in the deliverable document for this lab.

```{r ex="act3.3", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act3.3", type="sample-code"}
# Using the cafe data frame, find the iner-quartile range (IQR) for both age and bill.
```

### Activity 4: Standard Deviation

Using the *cafe* data frame, find the standard deviation for both *age* and *bill*. Include the answers to these problems in the deliverable document for this lab.

```{r ex="act3.4", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act3.4", type="sample-code"}
# Using the cafe data frame, find the standard deviation for both age and bill.
```

### Activity 5: Finding Outliers

Using the *cafe* data frame, find the boundary values for both the lower and upper outliers for *bill*, *tip*, and *miles*. Then, using a sort, determine if there are any outlier values beyond the boundaries in these three vectors (note: there may be no outliers). Include the boundary values and the outliers (if they exist) for all three variables in the deliverable document for this lab. (Note: do not include the full sorted data, only the values of the outliers.)

```{r ex="act3.5", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act3.5", type="sample-code"}
# Using the cafe data frame, find the cutoff values for the lower and upper outliers for bill, tip, and miles. Then, determine if there are any values outside those cutoffs.


```

## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 3," like "George Self Lab 3" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:03-lab03.Rmd-->

# Visualizing Descriptives {#lab4}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%", 
  message=FALSE,
  warnings=FALSE
)
```

```{r, include=FALSE}
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```

R makes it easy to calculate various data descriptives, as covered in [Lab 3](#lab3) and [Lab 4](#lab4); however, most people find it easier to understand data descriptives when those data are presented graphically. Fortunately, R has a great graphic tool for visualizing data descriptives: Boxplot (sometimes called a “Box and Whisker” plot). A Boxplot graphically illustrates Q1, the mean, the median, Q3, outlier boundaries, and outliers (if any are present).

## About Visualizations

This manual includes three different labs on data visualization because it is a critically important tool for data analysis. Visualizations are useful in two different phases of the analysis process: exploration and explanation. In the exploration phase, researchers are looking for interesting relationships in the data and those relationships are often difficult to detect in a table full of numbers but a visualization makes them instantly clear. As an example, here are two ways to look at the *trees$Volume* data.

```{r echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
print(trees$Volume)
```


The above table shows the measured volume for 31 Black Cherry Trees. Researchers looking at these numbers would not be able to detect very much. However, a simple box plot reveals a few interesting details, such as the presence of one upper outlier and that the data are positively skewed (the dark "median" line is low in the box).

```{r fig0401,fig.width=4.5,fig.height=4,fig.align="center", fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
boxplot(trees$Volume,
        main="Black Cherry Trees",
        ylab="Volume (cubic feet)"
     )
```


Visualizations like this make it easy to detect a patterns that are not obvious from the data table and researchers commonly use these types of visualizations in the *exploratory* phase of analysis. Not all visualizations are appropriate in the *explanatory* phase where research findings are revealed to the general public and for that phase of the project visualizations that are easier to understand should be employed. Researchers must carefully consider the many types of visualizations and which are most appropriate for exploration or explanation to be certain that the visualizations help rather than hinder understanding.

## Boxplots

Following is the summary data for *hp* from the *mtcars* data frame along with the boxplot for that same data.

```{r fig0402, fig.width=3.5,fig.height=4,out.width="40%", fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
summary(mtcars$hp)

boxplot(mtcars$hp, 
        main="MTCARS Data",
        ylab="Horsepower")
```

In the boxplot, the median is indicated by a dark line at 123, Q1 is 96.5 (the lower edge of the box) and Q3 is 180 (the upper edge of the box). The following equations show how the whiskers are placed for the boxplot. *Note: this calculation was first presented and explained in [Lab 3](#lab3outliers).*

$$
\begin{aligned}
Lower &= Q1 - (1.5 * IRQ) \\
Lower &= 96.5 - (1.5 * 83.5) \\
Lower &= 96.5 - 125.25 \\
Lower &= 0 
\end{aligned}
$$

Since the smallest value in the vector, 52, is larger than the calculated lower boundary, 0, the lower whisker is placed at 52.

$$
\begin{aligned}
Upper &= Q3 + (1.5 * IRQ) \\
Upper &= 180.0 + (1.5 * 83.5) \\
Upper &= 180.0 + 125.25 \\
Upper &= 305.25
\end{aligned}
$$

Since the calculated upper boundary, 305.25, is smaller than the largest value in the vector, 335.0, the upper whisker is placed at the largest data value that is smaller than or equal to 305.25, or 264 for this data vector.

The circle above the boxplot represents an outlier, which is 335 in this vector. If the data are a normal distribution then the whiskers will usually enclose all values in the vector and outliers will be rare.

### Generate a Simple Boxplot

The following script generates for different boxplots for four different variables in the *rock* data frame.

* Line 1: The *rock* data frame is attached to make the other functions in this script easier to write.
* Line 4: Print the summary information for the *area* vector.
* Line 5: Create the boxplot for *area*. Note that the boxplot includes a `main` attribute which adds a title above the boxplot.
* Lines 7-17 These are repetitions of Lines 3-5 for three other vectors in the *rock* data frame.
* Line 19 This detaches the *rock* data frame. This is not actually necessary since the script is ending anyway, but it is a good practice to always detach data frames.


```{r}
attach(rock)

# Summary for area
print(summary(area))
boxplot(rock$area, main="Rock Area")

# Summary for peri
print(summary(peri))
boxplot(peri, main="Rock Peri")

# Summary for shape
print(summary(shape))
boxplot(shape, main="Rock Shape")

# Summary for perm
print(summary(perm))
boxplot(perm, main="Rock Perm")

detach(rock)
```

Given these four boxplots, “Rock Peri” has the most symmetrical data since the box and whiskers are fairly equally distributed around the median, “Rock Perm” is the most skewed since so much of the plot is above the median, and “Rock Shape” has three upper outliers.

<div class="grsnote">
The DataCamp interface generates graphics in a *Plots* tab but because of the size of the interface those plots are "squished" and impossible to read. Click the double-headed arrow button on the *Plots* tab to open the graph in a larger window for evaluation and copying to a document. If the graphic does not open in a larger window then temporarily pause the browser's pop-up blocker.
</div>

## Outliers Revisited

A method to calculate the upper and lower boundaries for outliers was presented in [Lab 3](#lab3outliers); however, *R* includes a function that displays the values used to create a boxplot, including outliers, so laborious calculations are not necessary. As an example, consider the boxplot for *Rock Shape* that was created above.

```{r fig0403,fig.width=4.5,fig.height=4,fig.align="center", fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
attach(rock)

# Summary for shape
boxplot(shape, main="Rock Shape")

detach(rock)
```

To determine what values *R* used to generate the plot, attach the *rock* data frame and then enter `boxplot.stats(shape)` to get this result.


```{r boxPlot_stats, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', tut=FALSE}
attach(rock)

# Summary for shape
boxplot.stats(shape)

detach(rock)
```


This output has four different lines:


1. **$stats** These are the locations for the five horizontal lines in the plot, so the lower whisker is at 0.0903 on the y-axis, Q1 (the lower edge of the box) is at 0.1621 on the y-axis, the median (the heavy line in the middle of the box) is at 0.1988 on the y-axis, Q3 (the upper edge of the box) is at 0.2626 on the y-axis, and the upper whisker is at 0.3412 on the y-axis.

2. **$n** is the number of observations in the vector, or 48 in this case since 48 trees were measured.

3. **$conf** is the value on the y-axis that would be used to mark a 95% confidence level, but that statistic is not used in this lab.

4. **$out** These are the values of the outliers and the *rock shape* vector has three: 0.438712, 0,464125, and 0.420477. Thus, to find the outliers of a vector all that is needed is to use the fourth output line of the `boxplot.stats()` function. If there are no outliers then that line reports *numeric(0)* to indicate that there are zero outliers.

## Grouped Boxplots

Boxplots become much more useful when more than one data item is plotted side-by-side for comparison. For example, the following boxplots are helpful in determining if there is a difference in automobile weight by the number of cylinders in the engine.

```{r fig0404, fig.width=5,fig.height=4, fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
boxplot(wt ~ cyl, data = mtcars,
        ylab = "Weight (x1000 lbs)",
        xlab = "Number of Cylinders",
        main = "Weight by Cylinders")
```

By comparing the three boxplots it is easy to see that the more cylinders an engine has then the more the automobile weighs since the plots tend to be “higher” as the number of cylinders increases. Also notice that the plot for 8-cylinder cars does not have an upper whisker since it is exactly the same as Q3 but it does include outliers. It is also interesting to note that the whiskers for the three plots overlap, indicating, for example, that some 4-cylinder cars are heavier than some 6-cylinder cars.

## Color {#lab4color}

R permits designers to use a number of different color pallets to make a graph easier to understand. Here are the pallets available in base R:

```{r fig0405, fig.width=5,fig.height=5, fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
d <- as.table(c(1,1,1,1,1,1,1,1,1,1))
par(mfrow = c(5,1), mai = c(0.35,0.35,0.35,0.35), cex.main = 1.5)
barplot(d, col = cm.colors(10), axisnames = FALSE, axes = FALSE, main = "cm (cyan-magenta)")
barplot(d, col = heat.colors(10), axisnames = FALSE, axes = FALSE, main = "heat")
barplot(d, col = rainbow(10), axisnames = FALSE, axes = FALSE, main = "rainbow")
barplot(d, col = terrain.colors(10), axisnames = FALSE, axes = FALSE, main = "terrain")
barplot(d, col = topo.colors(10), axisnames = FALSE, axes = FALSE, main = "topo")

```


Adding the “heat” color pallet to the grouped boxplot generated earlier in this lab makes it easier to read due to the presence of colors.

```{r fig0406, fig.width=5,fig.height=4, fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
boxplot(wt ~ cyl, data = mtcars,
        ylab = "Weight (x1000 lbs)",
        xlab = "Number of Cylinders",
        main = "Grouped Boxplots With Heat Color", 
        col = heat.colors(3)
        )
```


When using color with graphics it is important for the researcher to keep two points in mind.

* First, it is estimated that about 8% of males and 0.5% of females are unable to distinguish between two or more colors, a condition that is often called “color blindness.”

* Second, if the research is ever printed in a black-and-white form then all color information is lost.

For these two reasons, it is probably best to not rely on color alone to provide information to the reader; rather, color should be used to enhance the understanding of a chart without being the sole source of information for that chart. 

## Generate Grouped Boxplots

Grouped boxplots are easy to create with R and the following script generates three examples.

* Lines 2-5: This is one long command that is broken over several lines to make it easier to read. Note that R does not require any sort of special “line continuation” character at the end of each line. As long as the parentheses started after the `boxplot` keyword does not close then R will continue reading that command on the next line.
* Line 2: *Temp ~ Month* This tells R to calculate the boxplot for the Temperature variable but group those temperatures by Month. It is important to remember the order for these two variables. First is the continuous data that should be analyzed and second is the grouping variable.
* Line 3: *data = airquality* In previous labs, the data frame name was prepended to the variable name using the $ operator or by attaching the data frame, and those methods could still be used for boxplots. However, many R functions are designed to enter only the variable names and then specify the data frame later in the function. In this case, the *airquality* data frame is identified as the source for the two variables being plotted.
* Line 4: *main = “Temp By Month”* This is the main title for the boxplot and is automatically printed in large font abovethe boxplot. R has a number of other formatting options available and several will be covered in later labs.
* Line 5: *col = rainbow(8)* This sets the color pallet to rainbow and instructs R to use eight colors from that pallet though the colors actually used are automatically selected by R. It is often useful to experiment with the number of colors requested from the pallet since the colors selected will change depending on the number requested and some combinations may be more useful than others.
* Lines 7-11: These are similar to Lines 2-5 but notice how the color pallet is selected. Selecting the “rainbow” pallet (Line 5) has a slightly different format than the other pallets.
* Lines 13-18: These lines are similar to Lines 7-11 but a new attribute was specified: *las = 2*. For this boxplot the groups are names of chicken feed rather than numbers or single letters. When those names are printed horizontally they “run into each other” and become unreadable. The *las = 2* specification turns those labels 90° so they do not interfere with each other.

```{r}
# Boxplot of airquality data
boxplot(Temp ~ Month, 
        data = airquality,
        main = "Temp By Month", 
        col = rainbow(8))

# Boxplot of warpbreaks data
boxplot(breaks ~ tension, 
        data = warpbreaks,
        main = "Breaks by Tension", 
        col = heat.colors(3))

# Boxplot of chickwts data
boxplot(weight ~ feed, 
        data = chickwts,
        main = "Chick Weight by Feed",
        col = topo.colors(6), 
        las = 2)
```

## Activities

<div class="grswarn">
Important Note: The DataCamp interface generates graphics in a *Plots* tab but because of the size of the interface those plots are "squished" and impossible to read. To generate a larger version of the graph for submission, click the double-headed arrow button along the bottom of the *Plots* tab to open the graph in a larger window and then copy/paste that larger image to the deliverable document for grading. If the graphic does not open in a larger window then temporarily pause the browser's pop-up blocker.
</div>

### Activity 1: Boxplot

Using the *cafe* data frame, generate a boxplot for both *age* and *bill*. Copy/paste the boxplots in the deliverable document for this lab.

```{r ex="act4.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act4.1", type="sample-code"}
# Using the cafe data frame, generate a boxplot for both age and bill.
```


### Activity 2: Outliers

Using the *cafe* data frame, find the outliers for *miles*. Record those outliers in the deliverable document for this lab.

```{r ex="act4.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act4.2", type="sample-code"}
# Using the cafe data frame, find the outliers for miles.
```

<!-- # Note: for my solution file, use this: boxplot.stats(miles)[[4]] -->


### Activity 3: Grouped Boxplot

Using the *cafe* data frame, generate a boxplot of *length* when grouped by *meal* to see if there is any difference in the length of the meal by the type of meal eaten (breakfast, lunch, dinner, other). Create a second boxplot of *tip* grouped by *sex* to see if there is any difference in the amount of tip left when grouped by sex. Copy/paste the boxplots in the deliverable document for this lab.

```{r ex="act4.3", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act4.3", type="sample-code"}
# Using the cafe data frame, generate a boxplot of length when grouped by meal


# Using the cafe data frame, generate a boxplot of tip grouped by sex
```

## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 4," like "George Self Lab 4" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:04-lab04.Rmd-->

# (PART) Counting {-}

# Frequency Tables {#lab5}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%",
  message=FALSE,
  warnings=FALSE
)

```

```{r, include=FALSE}
library(tutorial)
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```


Categorical data items are normally reported in frequency tables and crosstabs where the counts for a particular item are displayed. The only difference between these two types of tables is in the number of dimensions they display, frequency tables display only a single variable while a crosstab displays two variables. Both of these types of tables are commonly used to display polling data during the run-up to an election and would list things like the number of voters who would support some proposition (frequency table) or that same data broken out by party affiliation, sex, age, or some other category (crosstab). This lab explores both types of table.

## Frequency Tables

A frequency table is a one-dimensional table that lists a count of the number of times that some categorical data item appears in a vector. As an example, consider the following table which lists the
number of cars for each number of cylinders in the *mtcars* data frame.

```{r tut=FALSE}
table(mtcars$cyl)
```

This table shows that 14 cars in the data frame had 8 cylinders. Occasionally, researchers prefer to present percentages rather than raw numbers, like this for the cylinder data.

```{r tut=FALSE}
round(prop.table(table(mtcars$cyl))*100,2)
```

Thus, about 44% of the cars have eight cylinders.

Frequency tables are only useful for categorical data-type items. To illustrate why this is true, imagine creating a survey for all of the students at the University of Arizona and including "age" (continuous-type data) as one of the survey questions. Attempting to create a frequency table for the ages of the respondents would have, potentially, more than 65 columns since student ages would range from about 15 to more than 80 and each column would report the number of students for that particular age. While R could create a frequency table that large it would have so many columns that it would be virtually unusable. Normally, if continuous-type data need to be displayed in a table the data are grouped in some way, like ages 15-19, 20-24, etc, so there would be a manageable number of group counts to display.

As a second example, here are two frequency tables that list the number and percentage of states per region (from the *state.region* data frame).

```{r tut=FALSE}
table(state.region)

prop.table(table(state.region))
```

The following script illustrates a frequency table with two different options.

* Line 4: Create a simple frequency table listing the number of cars by the forward gears.
* Line 7: Create the same frequency table as Line 4 but add the total number of cars using the `addmargins` function.
* Line 10: Create the same frequency table as Line 4 but use a `prop.table` (proporitions) instead of counts. Appending "*100" to the end of this command multiplies the percents by 100 so they are easier for most people to understand (display "46.875" instead of "0.46875").

```{r tut=TRUE}
attach(mtcars)

# Count of cars by gears
table(gear)

# Count of cars with total
addmargins(table(gear))

# Percentage of cars by gears
prop.table(table(gear)) * 100

detach(mtcars)
```

## Crosstabs

A crosstab (sometimes called a contingency table or pivot table), is a table of frequencies used to display the relationship between two nominal or ordinal variables. As an example of a crosstab, consider a table listing *mtcars* by the number of forward gears and the number of cylinders.


```{r tut=FALSE}
xtabs(~gear+cyl, data=mtcars)

```

In this case, one car had a four cylinder engine and three forward gears while 12 cars had eight cylinders and three forward gears. By using a crosstab, a researcher can determine the frequency of some incident (number of cars) by two different criteria (gears and cylinders).

Here is a second example from the *esoph* data frame.

```{r tut=FALSE}
xtabs(~agegp+ncases, data=esoph)

```

Notice that there were few cases of esophageal cancer among people under the age of 45 but the number of cases increased between the ages of 45 and 74, with a peak in the 55-64 age group.

The following script demonstrates how to create crosstabs.

* Line 4: The `xtabs` command (for "Cross Tabs") creates a crosstab for the variables entered. It is important to notice the tilde character in this function. In many R commands the tilde is used to separate two parts of a formula. The first part are the data values to be acted upon and the second part are the categories used to group the values. In Line 7, there are no data values specified so R will simply count the number of times the various groups after the tilde show up. For example, "gear 3" and "cylinder 8" appear together 12 times in the data frame.
* Line 7: This is just another example of the `xtabs` command.
* Line 10: In this example, the *disp* variable comes before the tilde, so that variable contains the values to be acted upon. Since *cyl* is after the tilde, R will sum all displacements per number of cylinders, that is, *cyl* is the grouping variable. Thus, it will find the total displacement for cars with four cylinders, six cylinders, and eight cylinders. Frankly, this is of limited value. For example, R reports a total displacement of 1156.5 cubic inches for four cylinder cars but without knowing how many cars are in that category the number is meaningless.

```{r}
attach(mtcars)

# Count of gears and cylinders
xtabs(~gear+cyl)

# Count of carbs and cylinders
xtabs(~carb+cyl)

# Sum of displacement per cylinder
xtabs(disp~cyl)

detach(mtcars)
```

## Multi-Dimensional Crosstabs

Crosstabs can contain more than two dimensions. As an example, consider an experiment with pea plants where the amount of nitrogen, phosphorus, and potassium was varied to see what would happen to the crop yield. The *npk* data frame contains the result of that experiment and this crosstab displays that result in a multidimensional table.

```{r tut=FALSE}
xtabs(~N+P+K, data=npk)

```

When nitrogen (N) is 0, phosphorus (P) is 0, and potassium (K) is 0 the crop yield is 154.3 pounds/plot.

As a second example, consider the *mtcars* data frame. A researcher wanted to know if there is any relationship between the number of forward gears, the number of cylinders in the engine (that is, the size of the engine), and whether the car had a manual or automatic transmission. Here is the crosstab that was created.

```{r tut=FALSE}
xtabs(~cyl+gear+am, data=mtcars)

```

When *am* is 0 (which is the code for an automatic transmission) there were no cars with five forward gears and when *am* is 1 (manual transmission) there were no cars with three forward gears.

The following script demonstrates how to create multi-dimensional crosstabs.

* Line 4: The `xtabs` command (for "Cross Tabs") creates a crosstab for the variables entered. In this case, there is nothing on the left side of the tilde so R will return a count of the various categories. Since there are three grouping variables, *gear*, *cyl*, and *am* R will count instances for all three variables. For example, "gear 4" and "cylinder 4" appear together 2 times when the transmission is 0 and 6 times when the transmission is 1. 
* Line 7: This is just another example of multi-dimensional crosstab.

```{r}
attach(mtcars)

# Count of gears and cylinders by transmission
xtabs(~gear+cyl+am)

# Count of carbs and cylinders by engine type
xtabs(~carb+cyl+vs)

detach(mtcars)
```

## Calculated Crosstabs {#lab5calculatedcrosstabs}

Each of the crosstabs presented so far have displayed only counts of data; however, by using the `aggregate` function, crosstabs can also display calculated values, like the mean temperature for each summer month in the *airquality* data frame.

```{r tut=FALSE, include=FALSE}
#rm(mean) # I must have defined a variable named "mean" somewhere and that fouled up this function so I'm removing it here.
```

```{r agg02, tut=FALSE}
round(aggregate(Temp ~ Month, 
                data = airquality, 
                FUN = mean),2)
```

Though the above table has a number of calculated values the R command is only one line long.

As another example, the mean horsepower for automobiles can be calculated by the number of forward gears and engine cylinders from the *mtcars* data frame.

```{r agg01, tut=FALSE}
round(aggregate(hp ~ gear + cyl, 
          data = mtcars, 
          FUN = mean),2)
```

The following script demonstrates how to create calculated crosstab.

* Line 4: This line uses the `aggregate` command to calculate the mean displacement for each category of cylinders. Notice that this command starts with the tilde format where displacement will be aggregated for each category of cylinders, followed by the name of the dataset used (even if the dataset has been attached), and the statistical analysis to calculate. This aggregate function determines that four cylinder cars have a mean displacement of just over 105 cubic inches.
* Line 7: This line also calculates the mean displacement but groups the result by both number of cylinders and number of forward gears, creating a multi-dimensional crosstab. It calculates the mean displacement for four cylinder cars with three forward gears is just over 120 cubic inches.

```{r}
attach(mtcars)

# Mean of displacement by cylinders
aggregate(disp~cyl, mtcars, mean)

# Mean of displacement by cyl and gears
aggregate(disp~cyl+gear, mtcars, mean)

detach(mtcars)
```

Crosstabs and frequency tables are important reporting tools for researchers to master and R makes them easy to produce.

## Rounding ##

R makes it easy for a researcher to round the results to whatever level is desired. It is important to note that even if the displayed value is rounded, R still uses the full decimal number for calculations. The R command to round a number is `round()` where the number to be rounded is listed first in the parenthesis and the number of decimal places is listed second. So `round(1.3498, 2)` would be rounded to 1.35 but `round(1.3498, 1)` would be rounded to 1.3. The number to be rounded can be a calculated value rather than just a plain number. As an example, here is the same `aggregate` command used in the previous example, but with the results rounded to two decimal places. Notice how the `aggregate` command is the same, but it was wrapped in a `round` function to round off the calculations to two decimal places.

```{r}
attach(mtcars)

# Mean of displacement by cylinders
round(aggregate(disp~cyl, mtcars, mean),2)

# Mean of displacement by cyl and gears
round(aggregate(disp~cyl+gear, mtcars, mean),2)

detach(mtcars)
```


## Activities

### Activity 1: Frequency Tables

Using the *cafe* data frame, generate frequency tables for both *sex* and *meal*. Copy/paste the tables in the deliverable document for this lab.

```{r ex="act5.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act5.1", type="sample-code"}
# Using the cafe data frame, generate frequency tables for both sex and meal.
```


### Activity 2: Multi-Dimensional Crosstab

Using the *cafe* data frame, create a multi-dimensional crosstab of *food* and *svc* when grouped by *sex*. Copy/paste the table in the deliverable document for this lab.

```{r ex="act5.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act5.2", type="sample-code"}
# Using the cafe data frame, create a multi-dimensional crosstab of food and svc when grouped by sex.

```

### Activity 3: Calculated Crosstab

Using the *cafe* data frame, create a crosstab of the mean *tip* when grouped by *food* and *svc* with the means rounded to two decimal places. Copy/paste the table in the deliverable document for this lab.

```{r ex="act5.3", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act5.3", type="sample-code"}
# Using the cafe data frame, create a crosstab of the mean tip when grouped by food and svc with the means rounded to two decimal places.

```

## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 5," like "George Self Lab 5" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:05-lab05.Rmd-->

# Visualizing Frequency {#lab6}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%", 
  message=FALSE,
  warnings=FALSE
)

```

```{r, include=FALSE}
library(tutorial)
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```


Categorical data items are often reported using frequency tables where the number of times a particular survey item was selected is displayed. However, there are many ways to visualize frequency data and people using these charts and graphs often find it easier to understand the underlying data than with a table. 

## Visualizing Frequency

### Bar Chart

A bar chart is used to display the frequency count for categorical data. The following figure is a bar chart showing the number of automobiles with three, four, and five gears according to the mtcars data frame.

```{r fig0601,fig.width=3.5,fig.height=4,out.width="40%", fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
barplot(table(mtcars$gear), 
        main = "Count by Gears",
        xlab = "Number of Gears",
        ylab = "Number of Cars",
        col = rainbow(6)
        )
```

These types of visuals are more effective than a table full of numbers and they are easy to generate with R. The following script creates a simple bar chart.

* Line 4: This creates a bar chart using the `barplot` function. The first argument sent to the function is the data source for the chart. In this case, R creates a table from the *gears* variable in *mtcars* and then uses that table as data input for the bar chart. All of the other lines in this script embellish the bar chart to make it more usable.
* Line 5: The “main” attribute sets the main title for the bar chart. In general, for any graphic in R “main” is used to set the title of the graph.
* Line 6: This creates the label for the x-axis. 
* Line 7: This creates the label for the y-axis.
* Line 8: This sets the color palette for the graph. In this case, the “rainbow” palette is used for the graph. Three colors were requested from that palette but specifying any number larger than three would have worked and created a slightly different palette. Experimentation is needed to find the most suitable palette for any given graph. (Note: setting colors on graphs was introduced in [Lab 4](#lab4color))
* Line 9: This just closes the `barplot` function.

```{r}
# Simple Bar Chart
attach(mtcars)

barplot(table(gear),
  main = "Number of Cars By Gears",
  xlab = "Gears",
  ylab = "Count",
  col = rainbow(3)
)

detach(mtcars)
```

<div class="grsnote">
The DataCamp interface generates graphics in a *Plots* tab but because of the size of the interface those plots are "squished" and impossible to read. Click the double-headed arrow button on the *Plots* tab to open the graph in a larger window for evaluation and copying to a document. If the graphic does not open in a larger window then temporarily pause the browser's pop-up blocker.
</div>

### Clustered Bar Chart

A clustered bar chart (sometimes called a "Grouped Bar Chart") displays two or more variables and is used to display categorical data. In general, clustered bar charts are best at showing relationships between variables but not so good for determining the absolute size of each variable. The following chart shows the number of passengers on board the Titanic when it sank. While it is easy to determine that there were a lot more males than females on board, it is not possible to read the exact bar height of, for example, third class males.


```{r fig0602,fig.width=5.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
barplot(margin.table(Titanic,c(1,2)),
        beside = TRUE,
        main = "Titanic Passengers by Class and Sex",
        xlab = "Sex",
        ylab = "Count of Passengers",
        col = cm.colors(4)
        )
legend(x = "topright",
       y = c("First","Second","Third","Crew"),
       title = "Class",
       title.col = "navy",
       fill = cm.colors(4)
      )
```

The following script creates a clustered bar chart.

* Line 4: This begins the `barplot` function. It creates a table that contains the counts for *cyl* and *gear* in the *mtcars* data frame and then uses that table to produce the bar chart.
* Lines 5-8: These lines are essentially the same as for a single bar chart as described in the previous script.
* Line 9: Setting legend to TRUE displays a legend in the corner of the plot. Whenever more than one variable is being plotted it is important to display a legend for the reader. In this case, the legend displays the colors used for the *cyl* variable.
* Line 10: A stacked bar is the default type of chart but Line 10 instructs R to create a chart with the two variables beside each other.

```{r}
# Clustered Bar Chart
attach(mtcars)

barplot(table(cyl, gear),
  main = "Clustered Bar Chart",
  xlab = "Gear",
  ylab = "Count",
  col = rainbow(3),
  legend = TRUE,
  beside = TRUE
)

detach(mtcars)
```

### Clustered Bar Chart with Gradient Colors

For reasons described in the [Heat Map](#lab6heatmaps) section below, it is often important to display graphs that use only shades of one color. The following script produces the bar chart seen in the previous figure but using only shades of blue.

* Line 4: This line creates a new function named `colpal` (for “color palette”). That function calculates the codes for sevarl color gradients between blue and white. To create color gradients between green and white or red and black then those color names would be inserted in this line. In order to make the chart more usable for readers who are color blind, the gradients should contain only one color and either white or black.
* Line 6-9: These are the same as found in the previous bar chart script.
* Line 10: This line sets the color for this plot by calling the `colpal` function created in Line 4 and passing it a requirement for three colors.
* Lines 11-13: These are the same as found in the previous bar chart script.

```{r}
# Clustered Bar Chart With Gradient Colors
attach(mtcars)

colpal <- colorRampPalette(c("blue", "white"))

barplot(table(cyl, gear),
  main = "Clustered Bar Chart",
  xlab = "Gear",
  ylab = "Count",
  col = colpal(3),
  legend = TRUE,
  beside = TRUE
)

detach(mtcars)
```

### Stacked Bar Chart

A stacked bar chart has one variable stacked on top of another. In general, these are very difficult to read and should be avoided. Consider, for example, the following figure. This chart shows admissions for the University of California at Berkeley for six different programs. The top part of each bar (in blue) are the number admitted while the bottom part of each bar (in red) are the number rejected. Look at programs C and D. Were more students accepted in C or in D? Because these two values do not have the same baseline it is impossible to tell for certain which is larger.

```{r fig0603,fig.width=3,fig.height=3,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
barplot(margin.table(UCBAdmissions,c(1,3)),
        main = "Berkeley Admissions",
        xlab = "Program ID",
        ylab = "Count of Admissions",
        col = terrain.colors(3)
        )

```

The following script produces a stacked bar chart that uses only gradients of the color brown.

* Line 3: This creates up a function called `colpal` that makes gradients between brown and white for the bar chart.
* Lines 5-11 This is the `barplot` function used in the earlier scripts, except “beside = TRUE” is missing. By default, bar plots are stacked in R so if the “beside” argument is missing (or set to “FALSE”) then the result will be a stacked bar plot.

```{r}
# Stacked Bar Chart With Gradient Colors
attach(mtcars)

colpal <- colorRampPalette(c("Brown", "white"))

barplot(table(cyl,gear),
  main = "Automobile Cylinders vs. Gears",
  xlab = "Gears",
  ylab = "Count",
  col = colpal(3),
  legend = TRUE
  )

detach(mtcars)
```

It should be evident that the bar chart created in the above script is not very useful. While it is fairly easy to see that the number of 8-cylinder cars with three gears is much larger than the other categories, it is difficult to determine, for example, how many cars have five gears and eight cylinders. This difficulty is even worse when there are more than three levels for either of the two variables being plotted.

### Pie Chart

A pie chart is commonly used to display categorical data; however, pie charts are notoriously difficult to understand, especially if the writer uses some sort of 3-D effect or “exploded” slices. The human brain seems able to easily compare the heights of two or more bars, as in bar charts, but the areas of two or more slices of a pie chart are difficult to compare. For this reason, pie charts should be avoided in research reports. If they are used at all, they should only illustrate one slice’s relationship to the whole, not comparing one slice to another; and no more than four or five slices should ever be presented on one chart.

The following figure shows the results of an experiment to compare the effectiveness of various feed supplements on the growth rate of chickens. This figure illustrates the problem with pie charts. Notice that casein seems to promote growth better than horsebean, but it is impossible to determine if casein is better than sunflower from this chart.

```{r fig0604,fig.width=4,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
count <- aggregate(chickwts$weight, by=list(chickwts$feed), FUN=mean)
labels <- c(levels(chickwts$feed))
pie(count$x,
    labels = labels,
    main = "Chick Weights by Type of Feed",
    col = terrain.colors(6)
    )
```

The following script creates two pie charts.

* Line 4: This starts a pie chart function. In this line, the *feed* variable in the *chickwts* data frame is extracted to a table since the pie chart function expects input in the form of a table.
* Lines 5-6: These lines define the main title and colors used for the pie chart. These parameters are the same as was seen in other graphs in this lab.
* Line 7: This line specifies that the six levels of the feed variable will be used as the labels for the pie chart.
* Lines 11-14: This is the aggregate function first encountered in [Lab 5](#lab5calculatedcrosstabs). These lines create a new variable named *wts* that contains a table created by finding the mean weights for each type of feed. This variable contains the means that will be used to create the pie chart.
* Lines 16-20: This is the pie chart function that uses *wts* as input. By default, the aggregate function in Lines 11-14 creates a variable with two columns: *Group.1* and *x*. These variable names have no significance, they are just what R uses to generate the table. *Group.1* contains group names for each type of feed (that is, “casein,” “horsebean,” etc.) and *x* contains the mean weight for each of those types of feed.


```{r}
attach(chickwts)

# Count of Chicks by Feed
pie(table(feed),
  main = "Count of Chicks by Feed",
  col = rainbow(6),
  labels = c(levels(feed))
  )

# Mean Weight Gain of Chicks by Feed
wts <- aggregate(weight,
  by=list(feed),
  FUN=mean
  )

pie(wts$x,
  main = "Mean Chick Weight Gain By Feed",
  col = rainbow(6),
  labels = c(levels(feed))
  )

detach(chickwts)
```

Note: The above script includes two pie charts. The first pie chart only shows the number of chicks that were fed a given feed. Even though the pie “slices” are very nearly the same size there was a slightly different number of chicks on each type of feed. That information, though, would be of limited value since the goal of this research project was to determine which feed offered the best weight gain. The second pie chart attempts to answer that question. It is important for a researcher to keep the question being asked in mind so the answer provided addresses that question.

### Heat Maps {#lab6heatmaps}

Heat maps use colors to depict the counts of variables and are commonly found around election time to depict how precincts are voting, red for republican and blue for democrat. They are also routinely used on weather maps to depict areas with the greatest probability for rain or snow. Heat maps can be displayed in a geographical map where, for example, the various states are shaded to represent some factor, but they are also commonly seen as a grid. Designers must be careful to avoid using multiple hues on a heatmap and, instead, use only shades of the same color or gradients from one color to another. Using multiple hues creates what is sometimes called “clown’s pants” due to the extreme patchy color scheme. Charts with that type of coloration can be distracting and unusable. the following figure shows a heat map of various socioeconomic indicators by province in Switzerland from 1888.

```{r fig0605,fig.width=4,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
colpal <- colorRampPalette(c("blue", "white")) # generate a color pallette
x <- as.matrix(swiss[1:15,])
heatmap(x,
        Rowv=NA,
        Colv=NA,
        scale="column",
        margins=c(10,7),
        #col = colpal(10),
        main = "Swiss Stats",
        ylab = "Province",
        xlab = "Indicator" 
        )
```

In a heat map produced by R, lighter colors represent larger numbers. Thus, the province with the highest fertility rate is Franches-Mnt since it has the lightest color and the province with the least agriculture is Courtelary since it has the darkest color for those variables. Interpreting the heat map can be a challenge for the researcher. In some cases a light color would be positive and in others negative. For example, the highest education level would be in Neuveville (positive) but the highest infant mortality would be in Porrentruy (negative). Also, the colors are often very similar and difficult to distinguish. For example, for “examination” Cossonay has a numeric value of 22 while Aigle has 21. These two colors are slightly different but it would be difficult to detect that from the image. Often, the best that can be done with a heat map is identifying broad generalizations.

The following script creates a heat map for Party Size by Meal in the *cafe* data frame.

* Line 4: This line creates a contingency table from columns 4 and 8 of the *cafe* data frame and store that table in a variable named *htbl*. 
* Line 5: R can store data in several different formats and many, like vector and data frame, are used by other labs in this manual. Heat maps require data to be in a matrix format and this line converts the *htbl* contingency table into a matrix named *hmap*.
* Line 6: This is the start of the heat map function. This line instructs R to create a heat map from the *hmap* matrix.
* Line 7: The main title of the heatmap is “Cafe Ptysize by Meal.”
* Line 8: The x-axis is labeled “Meal.”
* Line 9: The y-axis is labeled “Ptysize.”
* Line 10: This suppresses the row “dendogram” that is used to order the rows. The best way to see what this line does is to comment it out and re-run the script.
* Line 11: This suppresses the column “dendogram.”
* Line 12: Sets the heat map to scale the columns. In this way, the color for each column cell is calculated such that the entire column’s mean is zero and the standard deviation is one. The other option is to scale “row” and researchers would want to try both to see which provides a better heat map.
* Line 13: This sets the right and bottom margins. The values were found by simple trial-and-error to produce the most legible heat map.

```{r ex="heatmap", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```


```{r heatmap, type="sample-code"}
# Cafe Party Size by Meal
attach(cafe)

htbl <- table(cafe[,c(8,4)])
hmap <- as.matrix(htbl)
heatmap(hmap,
  main = "Cafe Ptysize by Meal",
  xlab = "Meal",
  ylab = "Ptysize",
  Rowv=NA,
  Colv=NA,
  scale="column",
  margins=c(8,3)
)

detach(cafe)
```


### Mosiac Plot

A mosiac plot indicates the relative counts of items in a data frame by sizing areas on a grid. The following figure is a mosiac plot that indicates the relationship between the number of gears and cylinders in several cars. Notice that 8-cylinder cars overwhelmingly have three gears while 4-cylinder cars tend to have four gears. This plot gives a quick visual representation of the relationships between categorical variables, like a pie chart shows the relationship between continuous variables. A mosiac plot would suffer the same weaknesses as a pie chart and are, generally, rather difficult to interpret.

```{r fig0606,fig.width=3.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
colpal <- colorRampPalette(c("purple", "white")) # generate a color pallette
plot(table(mtcars$cyl,mtcars$gear),
     main = "US Car Information",
     xlab = "Cylinders",
     ylab = "Gears",
     col=colpal(4))
```

The following script creates a mosiac plot.

* Lines 4-9 The `plot` function is used to create mosaic plots. This same function is also used to create scatter plots, but that type of plot is discussed in [Lab 8](#lab8). Notice that a mosiac plot requires the input to be in table format so Line 4 creates a table from the *gear* and *cyl* variables. The other lines in this small script are similar to those used for other graphics functions and should be fairly easy to understand.


```{r}
# Mosiac Plots Using MTCars
attach(mtcars)

plot(table(gear, cyl),
  main = "Gears vs Cylinders",
  xlab = "Gears",
  ylab = "Cylinders",
  col = topo.colors(3)
  )

detach(mtcars)
```

## Activities

<div class="grswarn">
Important Note: The DataCamp interface generates graphics in a *Plots* tab but because of the size of the interface those plots are "squished" and impossible to read. To generate a larger version of the graph for submission, click the double-headed arrow button along the bottom of the *Plots* tab to open the graph in a larger window and then copy/paste that larger image to the deliverable document for grading. If the graphic does not open in a larger window then temporarily pause the browser's pop-up blocker.
</div>

### Activity 1: Bar Chart

Using the *cafe* data frame, create a bar chart for *meal*.  The chart should:

1. Have a title of “Activity 1: Bar Chart”
2. The x-axis should have this label: “Meal”
3. The y-axis should have this label: “Count”
4. Use any color palette desired

Copy/paste the bar chart in the deliverable document for this lab.

```{r ex="act6.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act6.1", type="sample-code"}
# Using the cafe data frame, create a bar chart for meal.

```

### Activity 2: Clustered Bar Chart

Using the *cafe* data frame, create a clustered bar chart that shows the number of people who ate a given *meal* by *sex*. The chart should have three clusters (male, female, other) and each of those three clusters should have four meals (breakfast, lunch, dinner, other). The chart should: 

1. Have a title of “Activity 2: Clustered Bar Chart”
2. The x-axis should have this label: “Sex 
3. The y-axis should have this label: “Count”
4. Use any color palette desired
5. There should be a legend that lists “breakfast, dinner, lunch,
other”
6. The bars should be beside each other rather than stacked.

Copy/paste the bar chart in the deliverable document for this lab.

```{r ex="act6.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act6.2", type="sample-code"}
# Using the cafe data frame, create a clustered bar chart that shows the number of people who ate a given meal by sex.

```

### Activity 3: Stacked Bar Chart

Using the *cafe* data frame, create a stacked bar chart that shows the number of people who ate a given *meal* by *sex*. The chart should have three columns (male, female, other) and each of those three columns should have four meals (breakfast, lunch, dinner, other). The chart should:

1. Have a title of “Activity 3: Stacked Bar Chart”
2. The x-axis should have this label: “Sex”
3. The y-axis should have this label: “Count”
4. Use any color palette desired
5. There should be a legend that lists “breakfast, dinner, lunch, other”
6. The bars should be stacked rather than beside each other.

Copy/paste the bar chart in the deliverable document for this lab.

```{r ex="act6.3", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act6.3", type="sample-code"}
# Using the cafe data frame, create a stacked bar chart that shows the number of people who ate a given meal by sex.

```

### Activity 4: Pie Chart

Using the *cafe* data frame, create a pie chart that shows the number of people who ate a given *meal* (breakfast, lunch, dinner, other). 

1. Have a title of “Activity 4: Pie Chart”
2. Use any color palette desired
3. Include labels for each of the pie slices

Copy/paste the pie chart in the deliverable document for this lab.

```{r ex="act6.4", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act6.4", type="sample-code"}
# Using the cafe data frame, create a pie chart that shows the number of people who ate a given meal.

```

### Activity 5: Heat Map

Using the *cafe* dataset, create a heat map that compares the number of people dining at each *meal* (breakfast, lunch, dinner, other) for each *day* (Monday, Tuesday, etc.). The goal is to find the most/least popular meal for each day of the week. (Tip: *meal* is column 4 and *day* is column 3 in the data frame.) The chart should:

1. Have a title of “Activity 5: Heat Map”
2. Have an xlab of “Meal”
3. Have a ylab of “Day”
4. Not include the Rowv/Colv symbols
5. Scale by column
6. Have margins=c(8,7)

Copy/paste the heat map in the deliverable document for this lab.

```{r ex="act6.5", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act6.5", type="sample-code"}
# Using the cafe dataset, create a heat map that compares the number of people dining at each meal for each day.

```

### Activity 6: Mosaic Plot

Using the *cafe* dataset, create a mosaic plot that compares the number of people dining at each *meal* (breakfast, lunch, dinner, other) to the rating for service (*svc*). The plot should:

1. Have a title of “Activity 6: Mosaic”
2. Have an xlab of “Meal”
3. Have a ylab of “Service”
4. Use any color palette desired

Copy/paste the mosaic plot in the deliverable document for this lab.

```{r ex="act6.6", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act6.6", type="sample-code"}
# Using the cafe dataset, create a mosaic plot that compares the number of people dining at each meal to the rating for service.

```

## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 6," like "George Self Lab 6" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:06-lab06.Rmd-->

# (PART) Relationships {-}

# Correlation and Regression {#lab7}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%", 
  message=FALSE,
  warnings=FALSE
)

```

```{r, include=FALSE}
library(tutorial)
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```


Correlation is a method used to describe a relationship between the independent (or *x-axis*) and dependent (or *y-axis*) variables in some research project. For example, imagine a project involving corn production where researchers applied a treatment to 50 acres of corn but not to another 50 acres in a nearby field. At the end of the growing season they found that the untreated field yielded 150 bushels per acre while the treated field yielded 170 bushels per acre. This would indicate a correlation, or relationship, between the treatment applied and the crop yield.

Regression analysis is a statistical method that uses correlation to find trends in data. With regression analysis, it is possible to predict the unknown value of the dependent variable based on a known value of the independent variable. For example, if a researcher recorded the 100 real estate sales in a small town along with the age of the houses being sold then it would be possible to use regression analysis to predict the selling price for a house when given its age. This lab explores both correlation and regression.

## Correlation

### Correlation and Causation

From the outset of this lab, it is important to remember that correlation does not equal causation. If two factors are correlated, even if that correlation is quite high, it does not follow that one is causing the other. As an example, if a research project found that students who spend more hours studying tend to get higher grades this would be an interesting correlation. However, that research, by itself, could not prove that longer studying hours causes higher grades. There would be other intervening factors that are not accounted for in this simple correlation (like the type of final examination used). As an egregious example of this point, consider that the mean age in the United States is rising (that is, people are living longer; thus, there are more elderly people) and that the crime of human trafficking is increasing. While these two facts may be correlated, it would not follow that old people are responsible for human trafficking! Instead, there are numerous social forces in play that are not accounted for in this simple correlation. It is important to keep in mind that correlation does not equal causation.

### Continuous Data

Pearson's Product-Moment Correlation Coefficient (normally called Pearson's *r*) is a measure of the strength of the relationship between two variables having continuous data that are normally distributed (they have bell-shaped curves). *(Note: [Lab 1](#lab1) contains information about various data types.)* Pearson's *r* is a number between -1.0 and +1.0, where 0.0 means there is no correlation between the two variables and either +1.0 or -1.0 means there is a perfect correlation. A positive correlation means that as one variable increases the other also increases. For example, as people age they tend to weigh more so a positive correlation would be expected between age and weight. A negative correlation, on the other hand, means that as one variable increases the other decreases. For example, as people age they tend to run slower so a negative correlation would be expected between age and running speed. In general, both the strength and direction of a correlation is indicated by the value of *r*:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Correlation = c("+.70 or higher", 
                  "+.40 to +.69", 
                  "+.30 to +.39",
                  "+.20 to +.29",
                  "+.19 to -.19",
                  "-.20 to -.29",
                  "-.30 to -.39",
                  "-.40 to -.69",
                  "-.70 or less"
                  ),
  Description = c(
    "Very strong positive",
    "Strong positive",
    "Moderate positive",
    "Weak positive",
    "No or negligible",
    "Weak negative",
    "Moderate negative",
    "Strong negative",
    "Very strong negative"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Correlation Descriptions") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center") %>%
  column_spec(1, bold = T, border_right = T)
```

The following examples are from the *mtcars* dataset and all involve continuous data, so Pearson's *r* was used to calculate the correlations. 

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Variables = c("disp-mpg",
                "wt-mpg",
                "wt-qsec",
                "disp-qsec",
                "drat-qsec"
                  ),
  Correlation = c(
    "-0.8476",
    "-0.8677",
    "-0.1747",
    "-0.4337",
    "+0.0912"
  ),
  Description = c(
    "Very Strong Negative",
    "Very Strong Negative",
    "No Correlation",
    "Strong Negative",
    "No Correlation"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Correlations of Continuous Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center"
                )

```

The following script demonstrates how to calculate Pearson's *r*.

* Line 2: This is the start of the `cor.test` function, which calculates the correlation between two variables. That function requires the *x-axis* variable be listed first then the *y-axis* variable.
* Line 3: This is a continuation of the `cor.test` function call and specifies the method to be Pearson's *r*. Since Pearson's *r* is the default method for the `cor.test` function this line did not need to be included but it is used in this example since the specification will be important in later examples in this lab.
* Lines 5-6: This is a second example of using `cor.test` to find the p-value and correlation between two continuous variables.

```{r}
# Pearson's r
cor.test(airquality$Wind, airquality$Ozone,
  method = "pearson")

cor.test(attitude$rating, attitude$complaints,
  method = "pearson")
```

The `cor.test` function returns a lot of information that will be important later in this lab and in Labs 9-10. However, here is an explanation for the result of the first test (the second is similar).

1. *Pearson's ...*: This is the title of the function being executed.
1. *data: ...*: This lists the two variables being correlated.
1. *t=...*: This line is the result of the calculation. The "t" score is used to calculate the p-value. The "df" are the "degrees of freedom" and is a measure of how many different levels the variables can take. The "p-value" is the probability value and, normally, a p-value less than 0.05 is considered significant. (Significance and p-value is discussed later in this lab.)
1. *alternative...*: The alternative hypothesis being tested. The default is that the correlation is not equal to zero and this line simply states the alternative hypothesis so the researcher can compare that hypothesis with the correlation and p-value to see if the null hypothesis can be rejected. ("Hypothesis"" is discussed in [Lab 9](#lab9).)
1. *95 percent...*: This shows the 95% confidence level for the true correlation. In this case, the true correlation should be between -0.706 and -0.471.
1. *sample estimates*: This begins the "estimates" section of the report.
1. *cor*: This verifies that the test executed was Pearson's *r* (Spearman's will report *rho* and Kendall's will report *tau*).
1. *-0.6015465*: This is the calculated correlation between the two variables.

### Categorical Data

When the one or both data elements are categorical then Spearman's *rho* or Kendall's *tau* is used to calculate the correlation. Other than the process used, the concept is exactly the same as for Pearson's *r* and the result is a correlation between -1.0 and +1.0 where the strength and direction of the correlation is determined by its value. Spearman's *rho* is used when at least one variable is ordered data and typically involves larger data samples while Kendall's *tau* can be used for any type of categorical data but is more accurate for smaller data samples. *(Note: "ordered data"" have categories that imply some sort of order but the difference between the categories cannot be calculated. As an example, a movie rated three stars is somehow better than one rated two stars but the difference between the two cannot be arithmetically calculated.)*

For example, imagine that a dataset included information about the age of people who purchased various makes of automobiles. If the "makes" are selected from a list (Ford, Chevrolet, Honda, etc.) then the data are categorical but no order is implied (that is, "Ford" is neither better or worse than "Chevrolet") so Kendall's *tau* would be used to calculate the correlation between the customers' preference for the make of an automobile and their ages. Perhaps the correlation would come out to +0.534 (this is a made-up number). This would indicate that there was a strong positive correlation between these two variables; that is, people tend to prefer a specific make based upon their age; or, to put it another way, as people age their preference for automobile make changes in a predictable way.

The following examples are from the *mtcars* dataset and all involve ordered data, so Spearman's *rho* was used to calculate the correlations.

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Variables = c("cyl—gear",
                "gear—am",
                "cyl—carb",
                "carb—gear",
                "vs—carb"
                  ),
  Correlation = c(
    "-0.5643",
    "+0.8077",
    "+0.5801",
    "+0.1149",
    "-0.6337"
  ),
  Description = c(
    "Strong Negative",
    "Very Strong Positive",
    "Strong Positive",
    "No Correlation",
    "Strong Negative"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Correlations of Categorical Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center"
                )

```

#### Spearman's Rho

The following script demonstrates using `cor.test` to calculate the correlations using Spearman's *rho*. The process for this calculation is the same as for Pearson's *r* except the method specified is "spearman". There is one other difference between this script and the first. Notice on line 2 that the x-axis variable, *esoph$agegp*, is inside an `as.numeric` function. Since the *agegp* variable actually uses phrases like "25-34" instead of a number it is necessary to convert that data to a number for Spearman's *rho* to analyze. (Note: This script will generate some warnings but they can be safely ignored for this lab.)

```{r}
# Spearman's rho
cor.test(as.numeric(esoph$agegp), esoph$ncases,
  method = "spearman")

cor.test(as.numeric(esoph$tobgp), esoph$ncases,
  method = "spearman")

```

The interpretation of the results of Spearman's *rho* is similar to that for Pearson's *r* and will not be further explained here.

#### Kendall's Tau

The following script demonstrates using `cor.test` to calculate the correlations using Kendall's *tau*. The process for this calculation is the same as for Pearson's *r* except the method specified is "kendall". As in the Spearman example, the first variable must be converted to numeric values. Also, this function will generate a warning but that can be ignored for this lab.

```{r}
# Kendall's tau
cor.test(as.numeric(npk$N), npk$yield,
  method = "kendall")

cor.test(as.numeric(warpbreaks$tension), warpbreaks$breaks,
  method = "kendall")

```

Interpreting Kendall's *tau* is similar to Pearson's *rho* and will not be further discussed here.

Pearson's *r*, Spearman's *rho*, and Kendall's *tau* all calculate correlation and it is reasonable to wonder which method should be used in any given situation. Here is a quick chart to help:

* Pearson's *r*: both data items being correlated are continuous.
* Spearman's *rho*: at least one variable is ordered and larger sample size.
* Kendall's *tau*: at least one variable is categorical (but not necessarily ordered) and smaller sample size.

Imagine a survey with a series of Likert scale items where respondents are asked to select one of five responses ranging from "Strongly Agree" to "Strongly Disagree" for statements like "I enjoyed the movie." Likert scales are ordered data and to determine how well the responses to these questions correlate with something like the respondents' ages, Spearman's *rho* would be an appropriate.

## Significance

Most people use the word *significant* to mean *important* but researchers and statisticians have a much different meaning for the word significant and it is vital to keep that difference in mind.

In statistics and research, significance means that the experimental results were such that they would not likely have been produced by mere chance. For example, if a coin is flipped 100 times, heads should come up 50 times. Of course, by pure chance, it would be possible for heads to come up 55 or even 60 times. However, if heads came up 100 times, researchers would suspect that something unusual was happening (and they would be right!). To a researcher, the central question of significance is "How many times can heads come up and still be considered just pure chance?"

In general, researchers use one of three significance levels: 1%, 5%, or 10%. A researcher conducting The Great Coin-Tossing Experiment may start by simply stating "This result will be significant at the 5% level." That would mean that if the coin were tossed 100 times, then anything between 47.5-52.5 (a 5% spread) "heads" tosses would be considered merely chance. However, 47 or 53 "heads" would be outside that 5% spread and would be significant.

It must seem somewhat subjective for a researcher to simply select the desired significance level, but an overwhelming number researchers in business and the social and behavioral sciences (like education, sociology, and psychology) tend to choose a significance level of 5%. There is no real reason for choosing a 5% level other than it is just the way things have traditionally been done for many years. Therefore, if a researcher selected something other than 5%, peer researchers would want some explanation concerning the "weird" significance level. Keep in mind, though, that statistical significance is not the same as practical significance. Wikipedia, that great repository of knowledge, includes this interesting example:

>As used in statistics, significant does not mean important or meaningful, as it does in everyday speech. For example, a study that included tens of thousands of participants might be able to say with great confidence that residents of one city were more intelligent than people of another city by 1/20 of an IQ point. This result would be statistically significant, but the difference is small enough to be utterly unimportant.

Note: the above statement was copied from Wikipedia in March, 2016; however, it has subsequently been removed from the article. Because it is still appropriate for this manual it was retained here. This was the original source of the quote: [http: //en.wikipedia.org/wiki/Statistical_significance](http: //en.wikipedia.org/wiki/Statistical_significance)

The calculated significance is typically reported as a p-value (for "probability value"). The following table contains the correlation and p-value for several pairs of variables from the *mtcars* data frame. 

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Variables = c(
    "cyl—vs",
    "wt—disp",
    "wt—qsec",
    "hp—drat",
    "am—hp"
  ),
  Correlation = c(
    "-0.8108",
    "+0.8880",
    "-0.1747",
    "-0.4488",
    "-0.2432"
  ),
  Pvalue = c(
    "1.843 x 10^-08^",
    "1.222 x 10^-11^",
    "0.3389",
    "0.009989",
    "0.1798"
  )
)

kable(cor_tbl, 
      "html",
      caption = "P-Values for Selected Correlations") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center"
                )

```

If a 5% significance level were specified for this data then any p-value smaller than 0.05 is considered significant; that is, the observed relationship is not likely due to chance. Given that, there is no significance in the correlation between *wt—qsec* and between *am—hp* since the p-values for those correlations are greater than 0.05. However, the correlations between the other variables are significant since the p-values for those are smaller than 0.05.

## Regression

A regression line can be drawn on a scatter plot to graphically show the relationship between two variables (this is sometimes called a "trend line" and a "line of best fit"). Moreover, if the data points in the scatter plot are all close to the regression line, then it indicates a strong correlation.

As an example, in the *mtcars* dataset the calculated Pearson's *r* for Quarter-mile Time and Horsepower is -0.708. These two variables have a strong negative correlation and are plotted below.

```{r fig0701,fig.width=4,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE}
plot(mtcars$hp, mtcars$qsec, 
     type="p", 
     main="Quarter-Mile Time \nby Horsepower", 
     xlab="Horsepower", 
     ylab="Time"
     )
# lm(y ~ x, data=d)
lmod <- lm(qsec ~ hp, data = mtcars)
abline(lmod,
       col = "red",
       lwd = 2
       )
```

The linear regression line drawn in the above figure has a slope of -0.01846 and a y-intercept of 20.55635. Using this information and the slope intercept formula (see the following equation), the quarter-mile time (y value) for a specific horsepower (x value) can be predicted. For example, imagine that the quarter-mile time for a horsepower of 200 is required:

$$
\begin{aligned}
y &= mx + b \\
y &= (-0.01846 * 200) + 20.55635 \\
y &= -3.692 + 20.55635 \\
y &= 16.86435
\end{aligned}
$$

In the equation, *m* is the slope of the regression line and *b* is the y-intercept. By plugging in the required value for *x* a simple calculation will determine the predicted value for *y*, which is 16.86 (rounded) in this example.

Regression analysis becomes less certain if the supplied *x* value is at the edge or outside the main body of the scatter plot. For example, using the above figure, it is mathematically possible to predict the quarter-mile time (y value) for a horsepower of 50:

$$
\begin{aligned}
y &= mx + b \\
y &= (-0.01846 * 50) + 20.55635 \\
y &= -0.923 + 20.55635 \\
y &= 19.63
\end{aligned}
$$

However, since the selected *x* value is outside the main body of the scatter plot then the calculated *y* value is suspect and should not be reported.

The following script calculates a predicted value for *y* when given an *x* value. 

* Line 3: The slope-intercept formula is noted in a comment on line 3.
* Line 5: This line executes the `lm` function to generate a "linear model" (the line of best fit) for *horsepower* (x-axis) and *quarter-mile time* (y-axis) in the *mtcars* data frame. The result of this function is stored in a variable named *lmod*. It is important to ensure that the variables are entered in the correct order or the linear model will be backwards. The dependent variable (the y-axis) is listed first and then the independent variable (the x-axis). Thus, it is expected that the quarter-mile time is dependent on the horsepower of the car, not the other way around.
* Lines 6-7: Each of these lines store one number in a variable for use in Line 13. There is an odd format used to extract each of these numbers. `lmod` contains a lot of information, including dozens of numbers, and Line 6 (the slope) accesses the `coef` section of `lmod` and extracts just the slope and then stores it in a variable named *b*. Line 7 accesses the `coef` section of `lmod` and extracts just the intercept and stores it in a variable named *m*.
* Line 10: The value of interest is stored in *x* for use in Line 13. In this case, the quarter-mile time for a car with 200 horsepower is being predicted
* Line 13: This is the slope-intercept formula written in R format. The formula is executed with the variables generated in Lines 6, 7, and 10 and the result is displayed on the screen. (Note, this result is slightly different from the one reported earlier in this lab due to rounding.)

```{r}
# Lab 07.04: Regression Analysis

# Slope-intercept formula: y = mx + b
# First, determine m and b
lmod <- lm(mtcars$qsec ~ mtcars$hp)
b <- coef(lmod)[[1]] # slope
m <- coef(lmod)[[2]] # intercept

# Now, specify the value of x
x <- 200.0

# Calculate the regression
(m * x) + b

```


## Activities

### Activity 1: Pearson's R

Using the *cafe* data frame, determine the correlation between the *length* of the meal and the *bill* to see if longer meals tend to cost more. Because these are both ratio variables, use Pearson's *r* as the correlation method. Record the correlation and p-value in the deliverable document for this lab.

```{r ex="act7.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act7.1", type="sample-code"}
# Using the cafe data frame, determine the correlation between the length of the meal and the bill.

```

### Activity 2: Spearman's Rho

Using the *cafe* data frame, determine the correlation between *age* and service (*svc*) to see if there is a relationship between a customer's age and their rating for the service. Because service is an ordered variable, use Spearman's *rho* as the correlation method. Record the correlation and p-value in the deliverable document for this lab.

```{r ex="act7.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act7.2", type="sample-code"}
# Using the cafe data frame, determine the correlation between age and service (svc).

```

### Activity 3: Kendall's Tau

Using the *cafe* data frame, determine the correlation between the distance driven (*miles*) and the *meal* eaten. Because meal is categorical but not ordered, use Kendall's *tau* as the correlation method. Record the correlation and p-value in the deliverable document for this lab.

```{r ex="act7.3", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act7.3", type="sample-code"}
# Using the cafe data frame, determine the correlation between miles and meal. 

```

### Activity 4: Prediction Using Regression

Using the *cafe* data frame, determine the slope and y-intercept for the size of *bill* (y-axis) as a function of the *length* (x-axis) of the meal. Use those numbers to predict the bill for a meal lasting 42 minutes. Round the bill to the nearest penny. Record the prediction in the deliverable document for this lab.

```{r ex="act7.4", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act7.4", type="sample-code"}
# Using the cafe data frame, predict the bill for a meal lasting 42 minutes.

```

### Activity 5: Prediction Using Regression

Using the *cafe* data frame, determine the slope and y-intercept for the size of the *tip* (y-axis) as a function of the *age* (x-axis) of the customer. Use those numbers to predict the tip from a customer who is 48 years old. Round the tip to the nearest penny. Record the prediction in the deliverable document for this lab.

```{r ex="act7.5", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act7.5", type="sample-code"}
# Using the cafe dataset, predict the tip from a 48 year-old customer.

```

## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 7," like "George Self Lab 7" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:07-lab07.Rmd-->

# Visualizing Data {#lab8}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%", 
  message=FALSE,
  warnings=FALSE
)

```

```{r, include=FALSE}
library(tutorial)
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```


[Lab 4](#lab4) and [Lab 6](#lab6) described various visualization techniques for descriptive and frequency data. This lab considers visualization techniques for quantitative data, which are usually continuous in nature.

## Visualizing Data

### Histogram

A histogram is a graph that shows the disribution of data that are continuous in nature, for example, age or height. A histogram resembles a bar chart but there is an important difference: a histogram is used for continuous data while a bar chart is used for categorical data. To emphasize that difference, histograms are normally drawn with no space between the bars (the data are continuous along the entire x-axis) while bar charts are normally drawn with a small space between bars (the data are categorical along the x-axis). As an example of a histogram, the next figure shows the New York city high temperature from May to September 1973 from the *airquality* data frame.

```{r fig0801,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}
hist(airquality$Temp,
     main = "NY Temperature",
     xlab = "Temperature",
     ylab = "Frequency",
     #labels = TRUE, # Add labels to each bar
     breaks = 12,
     freq = TRUE,
     col = cm.colors(9)
     )
```


Notice that there is not a separate bar for each temperature; rather, R has clustered five temperatures into a single bar. Thus, there is a bar that combines the temperatures 70-74 and not separate bars for each of those temperatures. This histogram helps researchers determine if temperature is normally distributed, that is, does it display a typical "bell" curve. While there are levels for each group of temperatures it should be fairly obvious that there are more temperatures around the 80° level than at either extreme and this histogram does indicate that the data are normally distributed.

As another example, the following figure shows a histogram for the body temperature of a beaver recorded every 10 minutes over the course of several hours. This histogram shows normally-distributed data though there is an outlier at a temperature of 37.6°.

```{r fig0802,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

hist(beaver1$temp,
     main = "Beaver Body Temperature",
     xlab = "Temperature",
     ylab = "Frequency",
     #labels = TRUE, # Add labels to each bar
     breaks = 9,
     freq = TRUE,
     col = cm.colors(13)
)

```


Histograms can also indicate data that are skewed and this would be important to researchers during a project’s exploratory phase. Consider, for example, the following figure which is the shape of petroleum rock samples in the rock data frame. While this histogram still indicates a normal distribution with levels falling off from a peak, there is a longer "tail" to the right so the data have a positive skew.

```{r fig0803,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

hist(rock$shape,
     main = "Rock Shape",
     xlab = "Peri / sq root area",
     ylab = "Frequency",
     #labels = TRUE, # Add labels to each bar
     breaks = 12,
     freq = TRUE,
     col = cm.colors(9)
)

```


Finally, consider the histogram in the following figure. This shows a bi-modal distribution where there are two clear peaks in the data. It is critical for researchers to know that the data are bi-modal before they begin analyzing it since having two modes can cause certain statistical tests to fail.

```{r fig0804,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

hist(rock$peri,
     main = "Rock Perimeter",
     xlab = "Perimeter",
     ylab = "Frequency",
     #labels = TRUE, # Add labels to each bar
     breaks = 16,
     freq = TRUE,
     col = cm.colors(10)
)

```

The following script creates an example histogram.

* Line 2: This is the beginning of the histogram function (it ends on Line 8). For this histogram the *Speed* variable from the *morley* data frame is specified as the data source for the histogram.
* Lines 3-5: This creates the main title of the histogram along with the labels for the x-axis and y-axis.
* Line 6 For a histogram, R must analyze the values contained in a variable and create "bins" for those values. That means that many of the continuous values will be grouped into a single bin for analysis. The "breaks" parameter tells R how many breaks to allow in the variable. In this case, eight breaks are specified, which would create nine bins. R will analyze the data and use the "breaks" parameter as a "suggestion"" and will only use that number of breaks if it makes sense for the data being graphed. Often, changing the number of breaks by just one or two will not change the histogram produced so researchers should play around with the "breaks" number to get the best possible representation of the data.
* Line 7: This specifies that 10 colors will be used from the "cm.color" palette to shade the various bars in the histogram. Researchers need to experiment a bit with the color palette and number of colors to get the best result; however, "cm.color" along with the number of bars in the histogram seems to work well.


```{r}
# Histogram
hist(morley$Speed,
  main = "Morley's Experiment",
  xlab = "Speed",
  ylab = "Frequency",
  breaks = 8,
  col = cm.colors(10)
)

```

<div class="grsnote">
The DataCamp interface generates graphics in a *Plots* tab but because of the size of the interface those plots are "squished" and impossible to read. Click the double-headed arrow button on the *Plots* tab to open the graph in a larger window for evaluation and copying to a document. If the graphic does not open in a larger window then temporarily pause the browser's pop-up blocker.
</div>

### Density Plot

A density plot provides the same information as a histogram but it is smoothed out so it is easier to read. Here is the same NY City temperature data from an earlier histogram but drawn as a density plot.

```{r fig0805,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

plot(density(airquality$Temp),
     main = "NY Temperature",
     xlab = "Temperature",
     ylab = "Density",
     lwd = 2,
     bty = "l"
)

```


Given the previous plot it is natural to wonder, "What is density?" This is a calculated value such that the total area under the curve is assumed to be one and then each point along the x-axis is calculated to contribute the correct proportional amount to that total density. For many purposes it is adequate to just consider a density plot to be a smoothed histogram. As just one other example, following is a density plot of the bi-modal histogram presented earlier. The density plot makes the bi-modal nature of the data very evident.

```{r fig0806,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

plot(density(rock$peri),
     main = "Rock Perimeter",
     xlab = "Perimeter",
     ylab = "Frequency",
     lwd = 2,
     bty = "l"
)

```

The following script creates a density plot of the *speed* variable in the *morley* data frame. The lines are very similar to the last script except R plots the `density` of the data in Line 2 rather than just the values.

```{r}
# Density Plot
plot(density(morley$Speed),
  main = "Morley's Experiment",
  xlab = "Speed",
  ylab = "Density",
  lwd = 2,
  col = "blue"
)
```

### Line Graph

Line graphs display the frequency of some value in a linear form that makes trend detection easier. These types of graphs are especially useful with what is called "time series" data; that is, data that are gathered over a long period of time. As an example, consider the following from the airmiles dataset which charts the number of passenger miles on US commercial airlines from 1937 to 1960.

```{r fig0807,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

plot(airmiles,
     main = "Passenger Air Miles",
     xlab = "Year",
     ylab = "Air Miles",
     type="l",
     lwd = 3,
     col = "blue"
     )

```

The following figure shows the number of accidental deaths in the United States from 1973 until 1979 by month taken from the *USAccDeaths* data frame. This line graph clearly shows a seasonal difference where there are more accidental deaths in the summer months than winter and detecting seasonal variation is one of the strengths of a line graph.

```{r fig0808,fig.width=6,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

plot(USAccDeaths,
     main = "US Accidental Deaths",
     xlab = "Year",
     ylab = "Number of Deaths",
     type="l",
     lwd = 2,
     col = "dark green"
)

```

As one final example, the following figure shows the approval rating for US Presidents from 1945 until 1975 taken from the *presidents* data frame. This line graph shows a very high approval rating in 1945 (Roosevelt at the end of WWII) with dips in the early 1950’s (Truman and the Korean Conflict) and about 1974 (Watergate and Nixon’s resignation). Notice that there are two gaps in the line (late 1940’s and 1973) caused by missing data in the data frame.

```{r fig0809,fig.width=6,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

plot(presidents,
     main = "US Presidential Approval",
     xlab = "Year",
     ylab = "Rating",
     type="l",
     lwd = 2,
     col = "purple"
)

```

Line graphs like those shown above are very useful for detecting a change in some variable, especially over time. The following script creates a line graph of the monthly number of lung disease deaths in the United Kingdom between 1974 and 1979 as found in the *ldeaths* data frame. It shows a very interesting cycle where there are consistently more deaths in the winter months than the summer months.

* Line 2: The `plot` function is called for the *ldeaths* data frame. By itself, `plot` will create a scatter plot but Line 8 in this script specifies a line graph.
* Lines 3-5: This creates the main title of the line graph along with the labels for the x-axis and y-axis.
* Line 6: This specifies the type of line graph wanted. The possible values are: "p" for points, "l" for lines, "b" for both, "c" for the lines part alone of "b", "o" for both ’overplotted,’ "h" for ’histogram’ like (or ’high-density’) vertical lines, "s" for stair steps, "S" for other steps, "n" for no plotting. Researchers could quickly try several different settings to determine the best way to present the data.
* Line 7: The *lwd* parameter sets the width of the line. The default value is 1 so this line specifies a double-width line in order to make it easier to see.
* Line 8: This sets the color of the line to red in order to set it off from the axis and text.

```{r}
# Line Graph
plot(ldeaths,
  main = "Lung Disease Deaths",
  xlab = "Month",
  ylab = "Number",
  type="l",
  lwd = 2,
  col = "red"
)
```

### Plot

Plots (often called "scatter plots") are used to show how two different variables are related. Scatter plots are often used in connection with correlation where they visually indicate the correlation between two variables. For example, The following figure is the plot of stopping distance vs. speed from the *cars* data frame.

```{r fig0810,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

plot(cars,
     main = "Stopping Distances",
     xlab = "Speed",
     ylab = "Stopping Distance"
     )
```

The previous figure shows that as a car’s speed increases the stopping distance also increases, which is exactly what would be expected. (Note: this data were gathered on cars in the 1920s.) Often, a line of best fit is included with a plot to better visualize the relationship between the two variables, as illustrated in the following figure.

```{r fig0811,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

plot(cars,
     main = "Stopping Distances",
     xlab = "Speed",
     ylab = "Stopping Distance"
)
abline(lm(cars$dist ~ cars$speed),
       col = "Blue",
       lwd = 2
       )
```

As a second example, The following figure is a plot of the eruption time for Old Faithful as a function of the waiting time between eruptions.

```{r fig0812,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

plot(faithful$eruptions ~ faithful$waiting,
     main = "Old Faithful Eruptions",
     xlab = "Waiting Time (Min)",
     ylab = "Eruption Time (Min)"
)
```

The previous figure shows that as the time between eruptions increases the time that the eruption lasts also increases. Notice that this scatter plot also suggests that the data are bi-modal since there are two clusters of points and a researcher would want to explore that matter before doing much else with the data.

The following script creates a simple plot using the *swiss* data frame.

* Line 2: This starts the `plot` function. Like many R functions, `plot` requires a formula input in the form of y ~ x where the dependent variable (the y-axis) is first in the formula and the independent variable (the x-axis) is second. For this plot, *Education* is the independent variable and will be on the x-axis while *Fertility* is the dependent variable on the y-axis. The researcher was answering the question "Does education level affect the number of children people have?"
* Lines 3-6: This creates the main title of the line graph, the labels for the x-axis and y-axis, and the color used for the plot points. This is similar to the `plot` scripts seen earlier in this lab.

```{r}
# Simple Plot
plot(swiss$Fertility ~ swiss$Education,
  main = "Swiss Indicators",
  xlab = "Education",
  ylab = "Fertility",
  col = "blue"
)
```

The following script creates two plots with lines of best fit.

* Lines 2-7: This starts the `plot` function which, like many in R, requires a formula input in the form of y ~ x where the dependent variable (the y-axis) is first in the formula and the independent variable (the x-axis) is second. For this plot, the *attitude* data frame is used and *rating* is the independent variable on the x-axis while *complaints* is the dependent variable on the y-axis. The researcher was answering the question "Do employees with higher overall ratings handle complaints better?" The parameters of the `plot` function are defined for several scripts in this lab and are not further discussed here.
* Line 8: This starts the `abline` function that ends on line 10. `Abline` overlays a line "from point A to point B" on an existing plot. In this case, the line to be drawn is calculated with the `lm` function, which calculates the slope and y-intercept of the line of best fit for the two specified vectors.
* Lines 9-10: These are the parameters for the line of best fit drawn on the plot.
* Lines 25-21: This is a second example of a plot with a line of best fit. This example uses Ozone and Wind from the *airquality* data frame.

```{r}
# Line of Best Fit
plot(attitude$complaints ~ attitude$rating,
main = "Attitude Data",
xlab = "Overall Rating",
ylab = "Complaints",
col = "dark blue"
)
abline(lm(attitude$complaints ~ attitude$rating),
col = "dark green",
lwd = 2
)

plot(airquality$Ozone ~ airquality$Wind,
main = "Air Quality",
xlab = "Wind Speed",
ylab = "Ozone Level",
col = "dark red"
)
abline(lm(airquality$Ozone ~ airquality$Wind),
col = "dark blue",
lwd = 2
)
```

### Q-Q Plot

Very often, researchers in the exploratory phase of a project need to know whether a data frame is normally distributed. Creating a histogram or density plot is very helpful in that regard, but a Q-Q
("Quantile-Quantile") Plot is a typical method used to determine if a data frame is normally distributed. The following figure is a Q-Q plot of the New York City temperatures in the *airquality* data frame.

```{r fig0815,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

qqnorm(airquality$Temp,
       main = "QQ Plot: NY City Temps"
       )
qqline(airquality$Temp,
       col = "Blue",
       lwd = 2
       )
```

An absolutely perfect normal distribution would generate a straight line Q-Q plot and the blue line in the previous figure is ideal. Interpreting a Q-Q plot is more art than science but as long as most values are near the ideal line then the data are considered normally distributed. The following figure is a Q-Q plot for the Old Faithful eruption. The dot plot for this data can be found earlier in this lab.

```{r fig0816,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

qqnorm(faithful$eruptions,
       main = "QQ Plot: Old Faithful"
)
qqline(faithful$eruptions,
       col = "Dark Green",
       lwd = 2
)
```

The above plot shows a typical bi-modal pattern. On the left side of the plot is a reasonably flat area from -3 to -0.5 on the x-axis and then the plot skips upward and creates a second reasonably flat area between 0.5 and 3. Imagine two parallel lines running through the lower and upper parts of the plot and then notice that the green "ideal" line does not get very close to the slope of either of those two lines. This Q-Q plot shows that the data are not normally distributed. The following figure shows a curved Q-Q plot that is typical for a data frame that is skewed. In this case, the plotted sunspot data have a heavy positive skew which is indicated by the long "tail" on the left side of the plot. That skew should be verified with a histogram or density plot.

```{r fig0817,fig.width=4.5,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, tut=FALSE}

qqnorm(sunspots,
       main = "QQ Plot: Sunspots"
)
qqline(sunspots,
       col = "Red",
       lwd = 2
)

```

The following script creates two Q-Q plots.

* Lines 2-4: This executes the `qqnorm` function and passes that function the weight variable from the *chickwts* data frame. This function draws the Q-Q Plot. The only parameter needed is *main*, which adds the title to the plot.
* Lines 5-7: This executes the `qqline` function and passes that function the weight variable from the *chickwts* data frame. This function draws the straight line that indicates a perfect Q-Q plot. The only parameters passed to the function in this script is to set the color to blue and the size to 2.
* Lines 10-16: This creates a second Q-Q plot for the Uptake variable in the CO2 data frame. The parameters are the same as for the first Q-Q plot except a different line color was used.

```{r}
# Q-Q Plots
qqnorm(chickwts$weight,
  main = "QQ Plot: Chick Weights"
)
qqline(chickwts$weight,
  col = "blue",
  lwd = 2
)

qqnorm(CO2$uptake,
  main = "QQ Plot: CO2 Uptake"
)
  qqline(CO2$uptake,
  col = "red",
  lwd = 2
)
```

## Activities

<div class="grswarn">
Important Note: The DataCamp interface generates graphics in a *Plots* tab but because of the size of the interface those plots are "squished" and impossible to read. To generate a larger version of the graph for submission, click the double-headed arrow button along the bottom of the *Plots* tab to open the graph in a larger window and then copy/paste that larger image to the deliverable document for grading. If the graphic does not open in a larger window then temporarily pause the browser's pop-up blocker.
</div>

### Activity 1: Histogram

Using the *cafe* data frame, create a histogram of *age*. The histogram should:

1. Have a title of "Activity 1: Histogram"
2. The x-axis should have this label: "Age"
3. The y-axis should have this label: "Frequency"
4. Use 10 breaks
5. Use any color palette desired

Copy/paste the histogram in the deliverable document for this lab.

```{r ex="act8.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act8.1", type="sample-code"}
# Using the cafe data frame, create a histogram of age.

```

### Activity 2: Density Plot

Using the *cafe* data frame, create a density plot of *age*. The plot should:

1. Have a title of "Activity 2: Density Plot"
2. The x-axis should have this label: "Age"
3. The y-axis should have this label: "Density"
4. Use a line width of 2
5. Use any color desired

Copy/paste the density plot in the deliverable document for this lab.

```{r ex="act8.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act8.2", type="sample-code"}
# Using the cafe data frame, create a density plot of age.

```

### Activity 3: Line Graph

Create a line graph of the *lynx* data frame. The plot should:

1. Have a title of "Activity 3: Line Graph"
2. The x-axis should have this label: "Year"
3. The y-axis should have this label: "Number"
4. Use a plot type of "l"
5. Use a line width of 2
6. Use the color "dark green"

Copy/paste the line graph in the deliverable document for this lab.

```{r ex="act8.3", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act8.3", type="sample-code"}
# Create a line graph of the lynx data frame. 

```

### Activity 4: Simple Plot

Using the *cafe* data frame, create a simple plot with *length* on the x-axis and *bill* on the y-axis to see if there is a relationship between the length of the meal and the bill. The plot should:

1. Have a title of "Activity 4: Simple Plot"
2. The x-axis should have this label: "Length of Meal"
3. The y-axis should have this label: "Bill"
4. Use the color "dark red"

Copy/paste the simple plot in the deliverable document for this lab.


```{r ex="act8.4", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act8.4", type="sample-code"}
# Using the cafe data frame, create a simple plot of length (x) vs. bill (y).

```

### Activity 5: Line of Best Fit

Using the *cafe* data frame, create a simple plot with *length* on the x-axis, *bill* on the y-axis, and a line of best fit to see if there is a relationship between the length of the meal and the bill. The plot should:

1. Have a title of "Activity 5: Line of Best Fit"
2. The x-axis should have this label: "Length of Meal"
3. The y-axis should have this label: "Bill"
4. Use the color "dark red"
5. Add a line of best fit with a width of 2 and color of "blue"

Copy/paste the plot in the deliverable document for this lab.

```{r ex="act8.5", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act8.5", type="sample-code"}
# Using the cafe dataset, create a simple plot of length (x) vs. bill (y) and a line of best fit.

```


### Activity 6: Q-Q Plot

Using the *cafe* data frame, create a Q-Q plot of *age*. The plot should:

1. Have a title of "Activity 6: Q-Q Plot"
2. A Q-Q line should be added with a color of blue and a weight of 2

Copy/paste the plot in the deliverable document for this lab.

```{r ex="act8.6", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act8.6", type="sample-code"}
# Using the cafe dataset, create a Q-Q plot of age.

```

## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 8," like "George Self Lab 8" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:08-lab08.Rmd-->

# (PART) Hypothesis Testing {-} 

# Parametric Hypothesis Testing {#lab9}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%",
  message=FALSE,
  warnings=FALSE
)

```

```{r, include=FALSE}
library(tutorial)
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```


Researchers should always begin a project with a hypothesis and then gather data to see if the hypothesis supports an underlying theory in a process commonly called the "scientific method." Continuous data gathered as part of the research project are analyzed using parametric techniques and two of the most commonly-used tests are described in this lab. This lab starts with a discussion of "hypothesis" since that is key to both parametric and nonparametric testing.

## Hypothesis {#lab9hypothesis}

A hypothesis is an attempted explanation for some observation and is often used as a starting point for further investigation. For example, imagine that a physician notices that babies born of women who smoke seem to weigh less than for women who do not smoke. That could lead to a hypothesis like "smoking during pregnancy is linked to lighter birth-weights." As another example, imagine that a restaurant owner notices that tipping seems to be higher on weekends than through the week. That might lead to a hypothesis that "the size of tips is higher on weekends than weekdays." After creating a hypothesis, a researcher would gather data and then statistically analyze those data to determine if the hypothesis is valid. Additional investigation may be needed to explain why that observation is true. In a research project, there are usually two related competing hypotheses:
the *Null Hypothesis* and the *Alternate Hypothesis*.

* Null Hypothesis (abbreviated H~0~). This is sometimes described as the "skeptical" view; that is, the explanation for some observed phenomena was mistaken. For example, the null hypothesis for the smoking mother observation mentioned above would be "smoking has no effect on a baby's weight" and for the tipping observation would be "there is no difference in tipping on the weekend."
* Alternate Hypothesis (abbreviated H~a~). This is the hypothesis that is being suggested as an explanation for the observed phenomenon. In the case of the smoking mothers mentioned, above the alternative hypothesis would be that smoking causes a decrease in birth weight. This is called the "alternate" because it is different from the status quo which is encapsulated in the null hypothesis.

One commonly-used example of the difference between the null and alternate hypothesis comes from the trial court system. When a jury deliberates about the guilt of a defendant they start from a position of "innocent until proven guilty," which would be the null hypothesis. The prosecutor is asking the jury to accept the alternate hypothesis, or "the defendant committed the crime."

For the most part, researchers will never conclude that the alternate hypothesis is true. There are always confounding variables that are not considered but could be the cause of some observation. For example, in the smoking mothers example mentioned above, even if the evidence indicates that babies born to smokers weigh less the researcher could not state conclusively that smoking caused that observation. Perhaps non-smoking mothers had better health care, perhaps they had better diets, perhaps they exercised more, or any of a number of other reasonable explanations not related to smoking. For that reason, the result of a research project is normally reported with one of two phrases:

* *The null hypothesis is rejected*. If the evidence indicates that there is a significant difference between the status quo and whatever was observed then the null hypothesis would be rejected. For the "tipping" example above, if the researcher found a significant difference in the amount of money tipped on weekends compared to weekdays then the null hypothesis (that is, tipping is the same on weekdays and weekends) would be rejected.

* *The null hypothesis cannot be rejected*. If the evidence indicates that there is no significant difference between the status quo and whatever was observed then the researcher would report that the null hypothesis could not be rejected. For example, if there was no significant difference in the birth weights of babies born to smokers and non-smokers then the researcher failed to reject the null hypothesis.

Often, a research hypothesis is based on a prediction rather than an observation and that hypothesis can be tested. Imagine a hypothesis like "walking one mile a day for one month decreases blood pressure." A researcher could easily test this by measuring the blood pressure of a group of volunteers, have them walk a mile every day for a month, and then measure their blood pressure at the end of the experiment to see if there was any significant difference.

## ANOVA

An Analysis of Variance (ANOVA) is used to analyze the difference in more than two groups of samples that are normally distributed. For example, imagine three groups of students were in the same class and one group was not required to attend tutoring, a second group was required to attending tutoring once a week, and a third group was required to attend tutoring two or more times a week. The null hypothesis (H~0~) is "The amount of tutoring does not significantly change a student's score on the final exam." The alternate hypothesis (H~a~) is "More frequent tutoring significantly changes a student's score on the final test." After the final exam was graded, an ANOVA could be administered and if that showed the test scores for those three groups of students had a significant difference then the null hypothesis would be rejected in favor of the alternate hypothesis.

The following one-line script generates an `ANOVA` from the *morley* data frame. `ANOVA` requires the two variables being compared to be input as a linear model (`lm`) formula in the form of y ~ x, where y is the dependent variable (continuous data) and x is the independent variable (the groups used to divide the continuous data). Also the data source is specified with a "data =" parameter.

```{r}
# ANOVA
anova(lm(Speed ~ Expt, data = morley))
```

The `ANOVA` function returns a lot of information, most of which is beyond the scope of this lab: 

```
> # ANOVA
> anova(lm(Speed ~ Expt, data = morley))
Analysis of Variance Table

Response: Speed
          Df Sum Sq Mean Sq F value    Pr(>F)
Expt       1  72581   72581  13.041 0.0004827 ***
Residuals 98 545444    5566
---
Signif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1 1
```

While an `ANOVA` function returns information that would be useful in a more thorough statistical analysis, this lab is only concerned with the p-value, 0.0004827, which is labeled "Pr(>F)" and is found near the end of Line 6. Following that p-value, R helpfully prints a code to aid in determining the significance of the result, three asterisks in this case. The last line in the results then lists the meaning of the various codes used. P-values that fall between 0 and 0.001 are marked with three asterisks, as in this case, so it is significant at the 0.1% level (0.001), the greatest possible level. If the p-value had been 0.03 then it would have been marked with one asterisk.

## T-test

A t-test is used to analyze the difference in two groups of samples that are normally distributed. One example of a t-test is to compare the spending habits of two similar groups of people. For example, do the residents of Tucson spend more on dining out than the residents of Phoenix? The null hypothesis (H~0~) is "People in Tucson and Phoenix spend the same amount of money when dining out." The alternate hypothesis (H~a~) is "People in Tucson and Phoenix spend different amounts of money when dining out." Imagine that the dining bills of 100 people from both cities were recorded and it was discovered that the mean bill in Phoenix is $15.13 and in Tucson is $12.47. If a t-test determines that there was a significant difference in those two numbers the null hypothesis would be rejected.

The following one-line script generates a `t.test` from the *npk* data frame. `t.test` requires the two variables being compared to be input as a formula in the form of y ~ x, where y is the dependent variable (continuous data) and x is the independent variable (the groups used to divide the continuous data). Also the data source is specified with a "data =" parameter.

```{r}
# T-Test (Independent)
t.test(yield ~ N, data = npk)

```

The `t.test` function returns a lot of information, most of which is beyond the scope of this lab: 

```
> # T-Test (Independent)
> t.test(yield ~ N, data = npk)

  Welch Two Sample t-test

data: yield by N
t = -2.4618, df = 21.88, p-value = 0.02218
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -10.3496778 -0.8836555
sample estimates:
mean in group 0 mean in group 1
       52.06667        57.68333
```

While a `t.test` function returns information that would be useful in a more thorough statistical analysis, this lab is only concerned with the p-value, 0.02218, which is found at the end of Line 6. On Line 7 R helpfully prints the alternative hypothesis and observes that the difference in the means for the two groups is not equal to 0, so the null hypothesis would be rejected.

## Activities

### Activity 1: ANOVA

Using the *cafe* data, conduct an ANOVA and report the p-value for the following variables. Note: in the document submitted for this lab, Activity 1 should have a simple listing, something like illustrated here. (Notes: these are not the correct answers to the listed tests. To indicate tiny values use scientific notation in a form like 1.6e-05 since that is easier to type.) Record the p-values in the deliverable document for this lab.

1. 6.352e-09
2. 0.0059
3. 4.028e-06
4. 0.3275

Here are the variables to test:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Num = c("1",
          "2",
          "3",
          "4"
          ),
  y = c(
    "Length",
    "Miles",
    "Age",
    "Tip"),
  x = c(
    "Day",
    "Party Size (ptysize)",
    "Food",
    "Service (svc)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "ANOVA Lab") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center") %>%
  column_spec(1, bold = T, border_right = T)
```

```{r ex="act9.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act9.1", type="sample-code"}
# Using the cafe data frame, conduct an ANOVA on the following variables.

# Length vs. Day

# Miles vs. Party Size (ptysize)

# Age vs. Food

# Tip vs. Service (svc)

```

### Activity 2: T-test

Using the *cafe* data, conduct a t.test (independent) and report the p-value for the following variables. Note: in the document submitted for this lab, Activity 2 should have a simple listing, something like illustrated here. (Notes: these are not the correct answers to the listed tests. To indicate tiny values use scientific notation in a form like 1.6e-05 since that is easier to type.) Record the p-values in the deliverable document for this lab.

1. 6.352e-09
2. 0.0059
3. 4.028e-06
4. 0.3275

Here are the variables to test:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Num = c("1",
          "2",
          "3",
          "4"
          ),
  y = c(
    "Miles",
    "Length",
    "Bill",
    "Tip"),
  x = c(
    "Recommend (recmd)",
    "Preference (pref)",
    "Recommend (recmd)",
    "Preference (pref)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "T.test Lab") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center") %>%
  column_spec(1, bold = T, border_right = T)
```

```{r ex="act9.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act9.2", type="sample-code"}
# Using the cafe data frame, conduct a T-test on the following variables.

# Miles vs. Recommend (recmd)

# Length vs. Preference (pref)

# Bill vs. Recommend (recmd)

# Tip vs. Preference (pref)

```


## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 9," like "George Self Lab 9" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:09-lab09.Rmd-->

# Non-Parametric Hypothesis Testing {#lab10}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%", 
  message=FALSE,
  warnings=FALSE
)

```

```{r, include=FALSE}
library(tutorial)
tutorial::go_interactive()

library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```


Researchers often begin a project with a hypothesis and then gather data to see if the hypothesis supports an underlying theory in a process commonly called the “scientific method.” Categorical data gathered as part of the research project are analyzed using nonparametric techniques and two of the most commonly-used tests are described in this lab. (Note: the concept of "hypothesis"" is discussed in [Lab 9](#lab9hypothesis).)

## Kruskal-Wallis H

This test is used to determine if there are any significant differences in three or more groups of data that are not normally-distributed, often categorical. Imagine that a researcher wanted to determine if there was a difference in smoking habit by age group. The subjects were interviewed and divided into three smoking groups: “Heavy” (more than one pack per day), “Light” (one pack or less per day), and “Nonsmokers.” They were also divided into age groups: <20, 20-29, 30-39, 40-49, >49. The researcher would then use a Kruskal-Wallis H test to see if there was a significant difference in smoking habit by age group.

The following one-line script generates a `Kruskal-Wallis H` from the *airquality* data frame. `kruskal.test` requires the two variables being compared to be input as a formula in the form of y ~ x, where y is the dependent variable (continuous data) and x is the independent variable (the groups used to divide the interval/ratio data). Also the data source is specified with a "data =" parameter.

```{r}
# Kruskal-Wallis H
kruskal.test(Ozone ~ Month, data = airquality)
```

The `kruskal.test` function returns a lot of information, most of which is beyond the scope of this lab: 

```
> # Kruskal-Wallis H
> kruskal.test(Ozone ~ Month, data = airquality)

  Kruskal-Wallis rank sum test

data: Ozone by Month
Kruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06
```

While a `kruskal.test` function returns information that would be useful in a more thorough statistical analysis, this lab is only concerned with the p-value, 6.901e-06, which is found at the end of Line 6. Because this is less than 0.05 (5%) it would be considered a significant result. Thus, the null hypothesis would be rejected (that there was no difference in Ozone by Month). Notice that this test does not indicate which month had the greatest ozone reading or if all months had some sort of significant variance from the mean, just that there is a significant difference between the months.

## Mann-Whitney U

This test is used to determine if there are any significant differences in two groups of data that are not normally-distributed, often categorical. Imagine that a movie producer wanted to know if there was a difference in the way the audience in two different cities responded to a movie. The null hypothesis (H~0~) is “There is no difference in movie-goers’ opinions between these two cities.” The alternate hypothesis (H~a~) is “Movie-goers’ opinions are significantly different by city.” As the audience members left the theater they would be asked to rate the movie on a scale of one to five stars. The ratings for the two cities would be collected and then a Mann-Whitney test would be used to determine if the difference in ratings between the cities was significant.

The following one-line script generates a `Mann-Whitney U` from the *CO2* data frame. In R, a
Mann-Whitney U test is automatically executed by the `wilcox.test` function whenever the grouping variable has only two levels. `wilcox.test` requires the two variables being compared to be input as a formula in the form of y ~ x, where y is the dependent variable (continuous data that is not normally distributed) and x is the independent variable (the groups used to divide the data). Also the data source is specified with a "data =" parameter.

```{r}
# Mann-Whitney U
wilcox.test(uptake ~ Treatment, data = CO2)
```

The `wilcox.test` function returns a lot of information, most of which is beyond the scope of this lab: 

```
> # Mann-Whitney U
> wilcox.test(uptake ~ Treatment, data = CO2)
Warning message: cannot compute exact p-value with ties

	   Wilcoxon rank sum test with continuity correction

data:  uptake by Treatment
W = 1187.5, p-value = 0.006358
alternative hypothesis: true location shift is not equal to 0

```

While a `wilcox.test` function returns information that would be useful in a more thorough statistical analysis, this lab is only concerned with the p-value, 0.006358, which is found at the end of Line 8. Because this is less than 0.05 (5%) it would be considered a significant result. There is also a warning printed in red font that starts on Line 3. That warning indicates that there are some repeated values, called “ties,” in the data. Because of those ties, the test did not report an exact p-value but, instead, one based on an approximation. The researcher would have to determine if an approximation is adequate, it usually is, and for this lab that warning can be ignored.

## Activities

### Activity 1: Kruskal-Wallis H

Using the *cafe* data, conduct a Kruskal-Wallis H and report the p-value for the following variables. Note: in the document submitted for this lab, Activity 1 should have a simple listing, something like illustrated here. (Notes: these are not the correct answers to the listed tests. To indicate tiny values use scientific notation in a form like 1.6e-05 since that is easier to type.) Record the p-values in the deliverable document for this lab.

1. 6.352e-09
2. 0.0059
3. 4.028e-06
4. 0.3275

Here are the variables to test:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Num = c("1",
          "2",
          "3",
          "4"
          ),
  y = c(
    "Tip",
    "Bill",
    "Party Size (ptysize)",
    "Length"),
  x = c(
    "Meal",
    "Day",
    "Recommend (recmd)",
    "Sex"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Kruskal-Wallis H Lab") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center") %>%
  column_spec(1, bold = T, border_right = T)
```

```{r ex="act10.1", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act10.1", type="sample-code"}
# Using the cafe data frame, conduct a Kruskal-Wallis H on the following variables.

# Tip vs. Meal

# Bill vs. Day

# Party Size (ptysize) vs. Recommend (recmd)

# Length vs. Sex

```

### Activity 2: Mann-Whitney U

Using the *cafe* data, conduct a Mann-Whitney U and report the p-value for the following variables. Note: in the document submitted for this lab, Activity 2 should have a simple listing, something like illustrated here. (Notes: these are not the correct answers to the listed tests. To indicate tiny values use scientific notation in a form like 1.6e-05 since that is easier to type.) Record the p-values in the deliverable document for this lab.

1. 6.352e-09
2. 0.0059

Here are the variables to test:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Num = c("1",
          "2"
          ),
  y = c(
    "Food",
    "Service (svc)"
  ),
  x = c(
    "Recommend (recmd)",
    "Preference (pref)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Mann-Whitney U Lab") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center") %>%
  column_spec(1, bold = T, border_right = T)
```

```{r ex="act10.2", type="pre-exercise-code"}
cafe <- read.csv('http://georgeself.com/maincafe.csv')
```

```{r ex="act10.2", type="sample-code"}
# Using the cafe data frame, conduct a Mann-Whitney U on the following variables.

# Food vs. Recommend (recmd)

# Service (svc) vs. Preference (pref)

```


## Deliverable

Complete the activities above and consolidate the responses into a single document. Name the document with your name and "Lab 10," like "George Self Lab 10" and submit that document for grade. It is also acceptable to consolidate the responses into a Google Document and submit a link to that document.


<!--chapter:end:10-lab10.Rmd-->

# (APPENDIX) Appendix {-} 

# Data Used In These Labs {#app01_data_dic}

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%",
  message=FALSE,
  warnings=FALSE
)

```

```{r, include=FALSE}
library(knitr)
library(kableExtra) # For building pretty tables
options(knitr.table.format = "html")

```

There are a number of data frames used in the lab exercises in this book and this appendix lists basic information about those data frames.

## airmiles

**Passenger Miles on Commercial US Airlines, 1937-1960**. The revenue passenger miles flown by commercial airlines in the United States for each year from 1937 to 1960. This is a time series of length 24; annually, 1937-1960 (from the FAA Statistical Handbook of Aviation).

## airquality

**New York Air Quality Measurements**. Daily air quality measurements in New York, May to September 1973. This is a data frame with 154 observations on 6 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c("Ozone", 
    "Solar.R", 
    "Wind",
    "Temp",
    "Month",
    "Day"
  ),
  Type = c(
    "int",
    "int",
    "numeric",
    "numeric",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Ozone (ppb)",
    "Solar R (lang)",
    "Wind (mpg)",
    "Temperature (degrees F)",
    "Month (1-12)",
    "Day of month (1-31)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Air Quality") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## attenu

**The Joyner-Boore Attenuation Data**. This data gives peak accelerations measured at various observation stations for 23 earthquakes in California. The data have been used by various workers to estimate the attenuating affect of distance on ground acceleration. This is a data frame with 182 observations on 5 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "event", 
    "mag", 
    "station",
    "dist",
    "accel"
  ),
  Type = c(
    "numeric",
    "numeric",
    "fac",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Event number",
    "Moment magnitude",
    "Station number (117 levels)",
    "Station-hypocenter distance (km)",
    "Peak acceleration"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Joyner-Boore Attenuation Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```


## attitude

**The Chatterjee-Price Attitude Data**. From a survey of the clerical employees of a large financial organization, the data are aggregated from the questionnaires of the approximately 35 employees for each of 30 (randomly selected) departments. The numbers give the percent proportion of favourable responses to seven questions in each department. This is a data frame with 30 observations on 7 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "ratings", 
    "complaints", 
    "privileges",
    "learning",
    "raises",
    "critical",
    "advance"
  ),
  Type = c(
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Overall rating",
    "Handling of employee complaints",
    "Does not allow special privileges",
    "Opportunity to learn",
    "Raises based on performance",
    "Too critical",
    "Advancement"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Chatterjee-Price Attitude Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## beaver

**Body Temperature Series of Two Beavers**. Reynolds (1994) describes a small part of a study of the long-term temperature dynamics of beaver Castor canadensis in north-central Wisconsin. Body temperature was measured by telemetry every 10 minutes for four females, but data from a one period of less than a day for each of two animals is used there. There are two data frames: beaver1 has 114 rows and 4 columns on body temperature measurements at 10 minute intervals and beaver2 has 100 rows and 4 columns on body temperature measurements at 10 minute intervals.

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "day", 
    "time", 
    "temp",
    "activ"
  ),
  Type = c(
    "numeric",
    "numeric",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Day of observation (days since 1990)",
    "Time of observation (0330 for 3:30 am)",
    "Measured body temperature (Celsius)",
    "Activity outside the retreat (0, 1)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Body Temperature Series of Two Beavers") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## cafe

*This is simulated data that is regenerated for each class. The number of observations will vary from 300 to 600 per class*. Customers of the Main Street Cafe completed surveys over a one week period. This is a data frame with about between 300 and 600 observations on 13 variables:


```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "sex", 
    "age", 
    "day",
    "meal", 
    "length",
    "miles", 
    "pref",
    "ptysize", 
    "food",
    "svc",
    "recmd",
    "bill", 
    "tip"
  ),
  Type = c(
    "fac",
    "int",
    "fac",
    "fac",
    "int",
    "int",
    "fac",
    "int",
    "int",
    "int",
    "fac",
    "numeric",
    "int"
  ),
  Description = c(
    "Sex (3 levels: male, female, other)", 
    "Age", 
    "Day (7 levels: Monday, Tuesday, etc.)",
    "Meal (4 levels: breakfast, lunch, dinner, other)", 
    "Length of meal (minutes)",
    "Miles driven to cafe", 
    "Seating preference (2 levels: booth, table)",
    "Number of people in party", 
    "Rating for food (ord levels: 1-5 'stars')",
    "Rating for service (ord levels: 1-5 'stars')",
    "Would recommend to a friend (2 levels: yes, no)",
    "Bill (dollars and cents)",
    "Amount of tip (whole dollars)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Cafe Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```


## cars

**Speed and Stopping Distances of Cars**. The data give the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s. This is a data frame with 50 observations on 2 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "speed", 
    "dist"
  ),
  Type = c(
    "numeric",
    "numeric"
  ),
  Description = c(
    "Speed (mpg)",
    "Stopping distance (ft)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Speed and Stopping Distances") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## chickwts

**Chicken Weights by Feed Type**. An experiment was conducted to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. This is a data frame with 71 observations on 2 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "weight", 
    "feed"
  ),
  Type = c(
    "numeric",
    "fac"
  ),
  Description = c(
    "Weight (unk units)",
    "Feed type"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Chicken Weights") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## CO2

**Carbon Dioxide Uptake in Grass Plants**. This data set is from an experiment on the cold tolerance of the grass species *Echinochloa crusgalli*. This is a data frame with 84 observations on 5 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "Plant", 
    "Type", 
    "Treatment",
    "conc", 
    "uptake"
  ),
  Type = c(
    "ordered",
    "fac",
    "fac",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Unique identifier of each plant (12 levels)", 
    "Origin of the plant (Quebec, Mississippi)", 
    "Type of treatment (nonchilled, chilled)",
    "Ambient CO2 concentratino (mL/L)", 
    "CO2 uptake rate (umol/sq meter/sec)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Carbon Dioxide Uptake in Grass Plants") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```


## esoph

**Smoking, Alcohol and (O)esophageal Cancer** . Data from a case-control study of (o)esophageal cancer in Ille-et-Vilaine, France. This is a data frame with 88 observations on 5 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "agegp", 
    "alcgp", 
    "tobgp",
    "ncases", 
    "ncontrols"
  ),
  Type = c(
    "ordered",
    "ordered",
    "ordered",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Age group (6 levels)", 
    "Alcohol consumption (4 levels)", 
    "Tobacco consumption (4 levels)",
    "Number of cases", 
    "Number of controls"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Smoking, Alcohol and (O)esophageal Cancer") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## faithful

**Old Faithful Geyser Data**. Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. This is a data frame with 272 observations on 2 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "eruptions", 
    "waiting"
  ),
  Type = c(
    "numeric",
    "numeric"
  ),
  Description = c(
    "Eruption time (minutes)",
    "Waiting time to next eruption (minutes)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Old Faithful Geyser Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## iris

**Edgar Anderson’s Iris Data**. This famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris: Iris setosa, versicolor, and virginica. This is a data frame with 150 observations on 5 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "Sepal.Length", 
    "Sepal.Width", 
    "Petal.Length",
    "Petal.Width", 
    "Species"
  ),
  Type = c(
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "factor"
  ),
  Description = c(
    "Length (cm)", 
    "Width (cm)", 
    "Length (cm)",
    "Width (cm)", 
    "Species (3 levels)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## LakeHuron

**Level of Lake Huron 1875-1972**. Annual measurements of the level, in feet, of Lake Huron 1875-1972. This is a time series of length 98.

## ldeaths

**Monthly Deaths from Lung Diseases in the UK**. Monthly deaths recorded from bronchitis, emphysema and asthma in the UK. This is a time series of length 72, 1974-1979.

## longley

**Longley’s Economic Regression Data**. This data set is a macroeconomic data set which provides a well-known example for a highly collinear regression. This is a data frame with 7 variables observed annually from 1947 to 1962 (n = 16):

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "GNP.deflator", 
    "GNP", 
    "Unemployed",
    "Armed.Forces", 
    "Population",
    "Year", 
    "Employed"
  ),
  Type = c(
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric"
  ),
  Description = c(
    "GNP implicit price deflator (1954=100)", 
    "Gross National Product", 
    "Number employed",
    "Number in armed forces", 
    "Population > 14 years of age",
    "Year", 
    "Number employed"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Longley’s Economic Regression Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## lynx

**Annual numbers of lynx trappings for 1821–1934 in Canada**. This is a time series of length 114, 1821-1934.

## morley

**Michelson Speed of Light Data**. A classical data of Michelson (but not this one with Morley) on measurements done in 1879 on the speed of light. The data consists of five experiments, each consisting of 20 consecutive "runs". The response is the speed of light measurement, suitably coded (km/sec, with 299,000 subtracted). This is a data frame with 100 observations on 3 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "Expt", 
    "Run", 
    "Speed"
  ),
  Type = c(
    "int",
    "int",
    "int"
  ),
  Description = c(
    "The experiment number, from 1 to 5", 
    "The run number within each experiment", 
    "Speed-of-light measurement"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Speed of Light") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## mtcars

**Motor Trend Car Road Tests**. The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). This is a data frame with 32 observations on 11 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "mpg", 
    "cyl", 
    "disp",
    "hp", 
    "drat",
    "wt", 
    "qsec",
    "vs", 
    "am",
    "gear",
    "carb"
  ),
  Type = c(
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Miles/(US) gallon", 
    "Number of cylinders", 
    "Displacement (cu. in.)",
    "Gross horsepower", 
    "Rear axle ratio",
    "Weight (1000 lbs)",
    "Quarter-mile time",
    "V8 (0) or straight (1) engine", 
    "Automatic (0) or manual (1) transmission",
    "Number of forward gears",
    "Number of carburetors"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Motor Trend Car Road Tests") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## npk

**Classical N, P, K Factorial Experiment**. A classical N, P, K (nitrogen, phosphate, potassium) factorial experiment on the growth of peas conducted on 6 blocks. Each half of a fractional factorial design confounding the NPK interaction was used on 3 of the plots. This is a data frame with 24 observations on 5 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "block", 
    "N", 
    "P",
    "K", 
    "yield"
  ),
  Type = c(
    "factor",
    "factor",
    "factor",
    "factor",
    "numeric"
  ),
  Description = c(
    "Which block (6 levels)", 
    "Indicator for nitrogen (No, Yes)", 
    "Indicator for phosphate (No, Yes)",
    "Indicator for potassium (No, Yes)", 
    "Yield of peas (pounds/plot)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "N, P, K Experiment") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```


## PlantGrowth

**Results from an Experiment on Plant Growth**. Results from an experiment to compare yields (as measured by dried weight of plants) obtained under a control and two different treatment conditions. This is a data frame with 30 observations on 2 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "weight", 
    "group" 
  ),
  Type = c(
    "numeric",
    "factor"
  ),
  Description = c(
    "Weight of yield (in)", 
    "Treatment group (3 levels)" 
  )
)

kable(cor_tbl, 
      "html",
      caption = "Plant Growth") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```


## presidents

**Quarterly Approval Ratings of US Presidents**. The (approximate) quarterly approval rating for the President of the United States from the first quarter of 1945 to the last quarter of 1974. This is a time series of length 120.

## rivers

**Lengths of Major North American Rivers**. This data set gives the lengths (in miles) of 141 "major" rivers in North America, as compiled by the US Geological Survey. This is a vector with 141 observations.

## rock

**Measurements on Petroleum Rock Samples**. This data set contains measurements on 48 rock samples from a petroleum reservoir. This is a data frame with 48 observations on 4 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "area", 
    "peri",
    "shape",
    "perm"
  ),
  Type = c(
    "int",
    "numeric",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Area of pores (pixels in 256 X 256)", 
    "Perimeter in pixels",
    "perimeter/square root of area",
    "Permeability in mili-Darcies"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Petroleum Rock Samples") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```


## sleep

**Student’s Sleep Data**. Data which show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients. This is a data frame with 20 observations on 3 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "extra", 
    "group",
    "ID"
  ),
  Type = c(
    "numeric",
    "fac",
    "fac"
  ),
  Description = c(
    "Increase in hours of sleep",
    "Drug given (2 levels)",
    "Patient ID (10 levels)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Sleep Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## state.region

**US State Facts and Figures**. Data sets related to the 50 states of the United States of America. This is a factor giving the state region (Northeast, South, North Central, West).

## sunspots

**Monthly Sunspot Numbers, 1749-1983**. Monthly mean sunspot numbers from 1749 to 1983. Collected at Swiss Federal Observatory, Zurich until 1960, then Tokyo Astronomical Observatory. This is a time series of length 2820.

## swiss

**Swiss Fertility and Socioeconomic Indicators (1888) Data**. Standardized fertility measure and socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888. This is a data frame with 47 observations on 6 variables, each of which is in percent (0-100):

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "Fertility", 
    "Agriculture", 
    "Examination",
    "Education", 
    "Catholic",
    "Infant.Mortality"
  ),
  Type = c(
    "numeric",
    "numeric",
    "int",
    "int",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Common fertility measure", 
    "% males involved in agriculture", 
    "% receiving high mark on army exam",
    "% education beyond primary school", 
    "% catholic (as opposed to protestant)",
    "Live births who live less than 1 year"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Swiss Socioeconomic Indicators (1888)") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## Titanic

**Survival of passengers on the Titanic**. This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner "Titanic," summarized according to economic status (class), sex, age and survival. This is a 4-dimensional array resulting from cross-tabulating 2201 observations on 4 variables. The variables and their levels are as follows:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Num = c(
    "1", 
    "2", 
    "3",
    "4"
  ),
  Name = c(
    "Class",
    "Sex",
    "Age",
    "Survived"
  ),
  Levels = c(
    "1st, 2nd, 3rd, Crew", 
    "Male, Female", 
    "Child, Adult",
    "No, Yes"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Titanic") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## trees

**Girth, Height and Volume for Black Cherry Trees**. This data set provides measurements of the girth, height and volume of timber in 31 felled black cherry trees. Note that girth is the diameter of the tree (in inches) measured at 4 ft 6 in above the ground. This is a data frame with 31 observations on 3 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "Girth", 
    "Height",
    "Volume"
  ),
  Type = c(
    "numeric",
    "numeric",
    "numeric"
  ),
  Description = c(
    "Tree diameter (inches)",
    "Tree height (feet)",
    "Volume of timber (cubic ft)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Black Cherry Tree Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## UCB Admissions

**Student Admissions at UC Berkeley**. Aggregate data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex. This is a 3-dimensional array resulting from cross-tabulating 4526 observations on 3 variables. The variables and their levels are as follows:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Num = c(
    "1", 
    "2",
    "3"
  ),
  Name = c(
    "Admit",
    "Gender",
    "Dept"
  ),
  Levels = c(
    "Admitted, Rejected",
    "Male, Female",
    "A, B, C, D, E, F"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Admissions at UC Berkeley") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

## USAccDeaths

**Accidental Deaths in the US 1973-1978**. The monthly totals of accidental deaths in the USA. This is a time series of length 72. 

## warpbreaks

**The Number of Breaks in Yarn During Weaving**. This data set gives the number of warp breaks per loom, where a loom corresponds to a fixed length of yarn. This is a data frame with 54 observations on 3 variables:

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Name = c(
    "breaks", 
    "wool",
    "tension"
  ),
  Type = c(
    "numeric",
    "fac",
    "fac"
  ),
  Description = c(
    "Number of breaks",
    "Type of wool (levels A, B)",
    "Tension on wool (levels L, M, H)"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Breaks in Yarn During Weaving") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center")
  # column_spec(1, bold = T, border_right = T)
```

# Creative Commons License {#app02_cc_lic}

## CC0 1.0 Universal

CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE LEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN ATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS INFORMATION ON AN "AS-IS" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES REGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED HEREUNDER.

### Statement of Purpose

The laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an "owner") of an original work of authorship and/or a database (each, a "Work").

Certain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works ("Commons") that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others.

For these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the "Affirmer"), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights.

1. __Copyright and Related Rights.__ A Work made available under CC0 may be protected by copyright and related or neighboring rights ("Copyright and Related Rights"). Copyright and Related Rights include, but are not limited to, the following:

    i. the right to reproduce, adapt, distribute, perform, display, communicate, and translate a Work;

    ii. moral rights retained by the original author(s) and/or performer(s);

    iii. publicity and privacy rights pertaining to a person's image or likeness depicted in a Work;

    iv. rights protecting against unfair competition in regards to a Work, subject to the limitations in paragraph 4(a), below;

    v. rights protecting the extraction, dissemination, use and reuse of data in a Work;

    vi. database rights (such as those arising under Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, and under any national implementation thereof, including any amended or successor version of such directive); and

    vii. other similar, equivalent or corresponding rights throughout the world based on applicable law or treaty, and any national implementations thereof.

2. __Waiver.__ To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer's Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the "Waiver"). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer's heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer's express Statement of Purpose.

3. __Public License Fallback.__ Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer's express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer's Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the "License"). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer's express Statement of Purpose.

4. __Limitations and Disclaimers.__

    a. No trademark or patent rights held by Affirmer are waived, abandoned, surrendered, licensed or otherwise affected by this document.

    b. Affirmer offers the Work as-is and makes no representations or warranties of any kind concerning the Work, express, implied, statutory or otherwise, including without limitation warranties of title, merchantability, fitness for a particular purpose, non infringement, or the absence of latent or other defects, accuracy, or the present or absence of errors, whether or not discoverable, all to the greatest extent permissible under applicable law.

    c. Affirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person's Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work.

    d. Affirmer understands and acknowledges that Creative Commons is not a party to this document and has no duty or obligation with respect to this CC0 or use of the Work.

> Creative Commons may be contacted at creativecommons.org



<!--chapter:end:11-app.Rmd-->

