# (PART) Relationships {-}

# Correlation and Regression

```{r 080.01, include = FALSE}

# Load some libraries
library(knitr)
library(kableExtra) # For building pretty tables
library(here)

# Set the rproj home directory
here::here()

# Set some knitr options
options(knitr.table.format = "html")
knitr::opts_chunk$set(
  collapse = TRUE,
  strip.white = TRUE,
  comment = "#>",
  out.width = "65%",
  message=FALSE,
  warnings=FALSE
)

# Turn on the DataCamp Light interactive tools
tutorial::go_interactive()

```

## Introduction

Correlation is a method used to describe a relationship between the independent (or _x-axis_) and dependent (or _y-axis_) variables in some research project. For example, imagine a project involving corn production where researchers applied a treatment to 50 acres of corn but not to another 50 acres in a nearby field. At the end of the growing season they found that the untreated field yielded 150 bushels per acre while the treated field yielded 170 bushels per acre. This would indicate a correlation, or relationship, between the treatment applied and the crop yield.

Regression analysis is a statistical method that uses correlation to find trends in data. With regression analysis, it is possible to predict the unknown value of the dependent variable based on a known value of the independent variable. For example, if a researcher recorded the 100 real estate sales in a small town along with the age of the houses being sold then it would be possible to use regression analysis to predict the selling price for a house when given its age. This lab explores both correlation and regression.

### Causation

From the outset of this lab, it is important to remember that correlation does not equal causation. If two factors are correlated, even if that correlation is quite high, it does not follow that one is causing the other. As an example, if a research project found that students who spend more hours studying tend to get higher grades this would be an interesting correlation. However, that research, by itself, could not prove that longer studying hours causes higher grades. There would be other intervening factors that are not accounted for in this simple correlation (like the type of final examination used). As an egregious example of this point, consider that the mean age in the United States is rising (that is, people are living longer; thus, there are more elderly people) and that the crime of human trafficking is increasing. While these two facts may be correlated, it would not follow that old people are responsible for human trafficking! Instead, there are numerous social forces in play that are not accounted for in this simple correlation. It is important to keep in mind that correlation does not equal causation.

### Definition

A correlation is a number between -1.0 and +1.0, where 0.0 means there is no correlation between the two variables and either +1.0 or -1.0 means there is a perfect correlation. A positive correlation means that as one variable increases the other also increases. For example, as people age they tend to weigh more so a positive correlation would be expected between age and weight. A negative correlation, on the other hand, means that as one variable increases the other decreases. For example, as people age they tend to run slower so a negative correlation would be expected between age and running speed. Here are the verbal definitions that are commonly accepted for a correlation's value.

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Correlation = c("+.70 or higher", 
                  "+.40 to +.69", 
                  "+.30 to +.39",
                  "+.20 to +.29",
                  "+.19 to -.19",
                  "-.20 to -.29",
                  "-.30 to -.39",
                  "-.40 to -.69",
                  "-.70 or less"
                  ),
  Description = c(
    "Very strong positive",
    "Strong positive",
    "Moderate positive",
    "Weak positive",
    "No or negligible",
    "Weak negative",
    "Moderate negative",
    "Strong negative",
    "Very strong negative"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Correlation Descriptions") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center") %>%
  column_spec(1, bold = T, border_right = T)
```

-----

## Pearson's r

Pearson's Product-Moment Correlation Coefficient (normally called Pearson's _r_) is a measure of the strength of the relationship between two variables having continuous data that are normally distributed (they have bell-shaped curves). _(Note: The [Introduction to R] tutorial contains information about various data types._ The following examples of correlation using Pearson's _r_ are from the [mtcars] dataset. 

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Variables = c("disp-mpg",
                "wt-mpg",
                "wt-qsec",
                "disp-qsec",
                "drat-qsec"
                  ),
  Correlation = c(
    "-0.8476",
    "-0.8677",
    "-0.1747",
    "-0.4337",
    "+0.0912"
  ),
  Description = c(
    "Very Strong Negative",
    "Very Strong Negative",
    "No Correlation",
    "Strong Negative",
    "No Correlation"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Correlations of Continuous Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center"
                )

```


### Demonstration: Pearson's r

The following script demonstrates how to calculate Pearson's _r_.

* Line 2: This is the start of the `cor.test` function, which calculates the correlation between two variables. That function requires the _x-axis_ variable be listed first then the _y-axis_ variable.
* Line 3: This is a continuation of the `cor.test` function call and specifies the method to be Pearson's _r_. Since Pearson's _r_ is the default method for the `cor.test` function this line did not need to be included but it is used in this example since the specification will be important in later examples in this lab.

```{r 080.04}

# Pearson's r
cor.test(airquality$Wind, airquality$Ozone,
  method = "pearson")

```

The `cor.test` function returns a lot of information that will be important in later tutorials; however, here is an explanation for the result of this function.

1. **Pearson's product-moment correlation**: This is the title of the function being executed.
1. **data: ...**: This lists the two variables being correlated.
1. **t=...**: This line is the result of the calculation. 
    * The "t" score is used to calculate the p-value at the end of the line. 
    * The "df" are the "degrees of freedom" and is a measure of how many different levels the variables can take. 
    * The "p-value" is the probability value and, normally, a p-value less than 0.05 is considered significant. (Significance and p-value are discussed later in this tutorial.)
1. **alternative...**: The alternative hypothesis being tested. The default is that the correlation is not equal to zero and this line simply states the alternative hypothesis so the researcher can compare that hypothesis with the correlation and p-value to see if the null hypothesis can be rejected. ("Hypothesis" is discussed in the [Parametric Hypothesis Testing] tutorial.)
1. **95 percent...**: This shows the 95% confidence level for the true correlation. In this case, the true correlation should be between -0.706 and -0.471.
1. **sample estimates**: This begins the "estimates" section of the report.
1. **cor**: This verifies that the test executed was Pearson's _r_ (Spearman's will report _rho_ and Kendall's will report _tau_).
1. **-0.6015465**: This is the calculated correlation between the two variables.

### Guided Practice: Pearson's r

Using the [CO2] data frame, calculate Pearson's _r_ for the variables _conc_ and _uptake_.

```{r 080.05, ex="sc080.01", type="pre-exercise-code"}

# No pre-exercise code for this exercise

```

```{r 080.06, ex="sc080.01", type="sample-code"}
# Calculate Pearson's r for conc and uptake in the CO2 data frame.


```

```{r 080.07, ex="sc080.01", type="solution"}

cor.test(CO2$conc, CO2$uptake,
  method = "pearson")

```

```{r 080.08, ex="sc080.01", type="sct"}

ex11neq <- "You should specify CO2$conc and CO2$uptake as the variables. Note: CO2 are capital letters. Be sure the method specified is pearson."

ex() %>%
  check_function("cor.test") %>%
  check_result() %>%
  check_equal(incorrect_msg = ex11neq)

success_msg("Perfect! Correlations are one of the most important tools available to researchers.")

```


### Activity: Pearson's r

Using the [cafe] data frame, determine the correlation between the _length_ of the meal and the _bill_ to see if longer meals tend to cost more. Because these are both ratio variables, use Pearson's _r_ as the correlation method. Record the correlation in the deliverable document for this lab.

```{r 080.09, ex="act080.01", type="pre-exercise-code"}

cafe <- read.csv('https://labs.basv316.com/cafe.csv')

```

```{r 080.10, ex="act080.01", type="sample-code"}
# Using the cafe data frame, determine the correlation between the length of the meal and the bill.



```

-----

## Categorical Data

When the one or both data elements are categorical then Spearman's _rho_ or Kendall's _tau_ is used to calculate the correlation. Other than the process used, the concept is exactly the same as for Pearson's _r_ and the result is a correlation between -1.0 and +1.0 where the strength and direction of the correlation is determined by its value. Spearman's _rho_ is used when at least one variable is ordered data and typically involves larger data samples while Kendall's _tau_ can be used for any type of categorical data but is more accurate for smaller data samples. _(Note: "ordered data" have categories that imply some sort of order but the difference between the categories cannot be calculated. As an example, if some people strongly agree with a statement and others strongly disagree there is an obvious difference in those opinions but it would be impossible to arithmetically calculate that difference.)_

For example, imagine a research project that is attempting to determine movie preference by a person's age. The researcher could ask a large group of people to indicate their age and how well they liked certain types of movies by giving each a "star" rating. Clearly, types that are rated five-star are somehow better than those rated four-star, but it is not possible to quantify that difference in any sort of meaningful way. Spearman's _rho_ would be used to calculate a correlation between age and movie rating. If that correlation came out to -0.632 (this is a made-up number) for "horror" movies then the researcher could conclude that there is a negative relationship between age and preference for horror movies. That is, as people age they tend to not prefer horror movies.

On the other hand, imagine that a dataset included information about the age of people who purchased various makes of automobiles. If the "makes" are selected from a list (Ford, Chevrolet, Honda, etc.) then the data are categorical but no order is implied (that is, "Ford" is neither better or worse than "Chevrolet") so Kendall's _tau_ would be used to calculate the correlation between the customers' preference for the make of an automobile and their ages. Perhaps the correlation would come out to +0.534 (this is a made-up number). This would indicate that there was a strong positive correlation between these two variables; that is, people tend to prefer a specific make based upon their age; or, to put it another way, as people age their preference for automobile make changes in a predictable way.

The following examples are from the [mtcars] dataset and since they all involve ordered data Spearman's _rho_ was used to calculate the correlations.

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Variables = c("cyl—gear",
                "gear—am",
                "cyl—carb",
                "carb—gear",
                "vs—carb"
                  ),
  Correlation = c(
    "-0.5643",
    "+0.8077",
    "+0.5801",
    "+0.1149",
    "-0.6337"
  ),
  Description = c(
    "Strong Negative",
    "Very Strong Positive",
    "Strong Positive",
    "No Correlation",
    "Strong Negative"
  )
)

kable(cor_tbl, 
      "html",
      caption = "Correlations of Categorical Data") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center"
                )

```

-----

### Demonstration: Spearman's rho

The following script demonstrates using `cor.test` to calculate correlations from the [esoph] data frame using Spearman's _rho_. The process for this calculation is the same as for Pearson's _r_ except the method specified is "spearman". There is one other difference between this script and the first. <span class="grshilite">Notice on line 2 that _esoph$agegp_ is inside an as.numeric function. Since _agegp_ uses text like "25-34" instead of a number this converts that to a number for Spearman's _rho_.</span> (Note: This script will generate a warnings about p-values but that can be safely ignored for this tutorial.)

```{r 080.12}

# Spearman's rho
cor.test(as.numeric(esoph$agegp), esoph$ncases,
  method = "spearman")

```

The interpretation of the results of Spearman's _rho_ is similar to that for Pearson's _r_ and will not be further explained here.

### Guided Practice: Spearman's rho

Using the [CO2] data frame, calculate Spearmans's _rho_ for the variables _Plant_ (which is ordered) and _uptake_.

```{r 080.13, ex="sc080.02", type="pre-exercise-code"}

# No pre-exercise code for this exercise

```

```{r 080.14, ex="sc080.02", type="sample-code"}
# Calculate Spearman's rho for Plant and uptake in the CO2 data frame.


```

```{r 080.15, ex="sc080.02", type="solution"}

cor.test(as.numeric(CO2$Plant), CO2$uptake,
  method = "spearman")

```

```{r 080.16, ex="sc080.02", type="sct"}

ex21neq <- "You should specify the as.numeric function changes the CO2$Plant variable. Notice the capital letters: CO and P."
ex22neq <- "You should specify as.numeric(CO2$Plant) and CO2$uptake as the variables. Note: CO and P are capital letters."
ex23neq <- "Be sure the method specified is spearman."
ex24neq <- "Check the input variables. Did you specify CO2$Plant and CO2$uptake?"


state21 <- ex() %>% check_function("as.numeric")
state21 %>%
  check_arg("x") %>%
  check_equal(incorrect_msg = ex21neq)

state22 <- ex() %>% check_function("cor.test")
state22 %>%
  check_arg("x") %>%
  check_equal(incorrect_msg = ex22neq)
state22 %>%
  check_arg("method") %>%
  check_equal(incorrect_msg = ex23neq)

ex() %>%
  check_function("cor.test") %>%
  check_result() %>%
  check_equal(incorrect_msg = ex24neq)

success_msg("Perfect! Correlations are one of the most important tools available to researchers.")

```

### Activity: Spearman's rho

Using the [cafe] data frame, determine the correlation between _age_ and service (_svc_) to see if there is a relationship between a customer's age and their rating for the service. Because service contains ordered data, use Spearman's _rho_ as the correlation method. Record the correlation in the deliverable document for this lab.

```{r 080.17, ex="act080.02", type="pre-exercise-code"}

cafe <- read.csv('https://labs.basv316.com/cafe.csv')

```

```{r 080.18, ex="act080.02", type="sample-code"}
# Using the cafe data frame, determine the correlation between age and service (svc).



```

-----

### Demonstration: Kendall's tau

The following script demonstrates using `cor.test` to calculate correlations from the [npk] data frame using Kendall's _tau_. The process for this calculation is the same as for Pearson's _r_ except the method specified is "kendall". As in the Spearman example, the first variable must be converted to numeric values. Also, this function will generate a warning but that can be ignored for this lab.

```{r 080.19}
# Kendall's tau
cor.test(as.numeric(npk$N), npk$yield,
  method = "kendall")

```

Interpreting Kendall's _tau_ is similar to Pearson's _r_ and will not be further discussed here.

### Guided Practice: Kendall's tau

Using the [CO2] data frame, calculate Kendall's _tau_ for the variables _Type_ and _uptake_.

```{r 080.20, ex="sc080.03", type="pre-exercise-code"}

# No pre-exercise code for this exercise

```

```{r 080.21, ex="sc080.03", type="sample-code"}
# Calculate Kendall's tau for Type and uptake in the CO2 data frame.


```

```{r 080.22, ex="sc080.03", type="solution"}

cor.test(as.numeric(CO2$Type), CO2$uptake,
  method = "kendall")

```

```{r 080.23, ex="sc080.03", type="sct"}

ex31neq <- "You should specify the as.numeric function changes the CO2$Type variable. Notice the capital letters: CO and T."
ex32neq <- "You should specify as.numeric(CO2$Type) and CO2$uptake as the variables. Note: CO and T are capital letters."
ex33neq <- "Be sure the method specified is kendall."
ex34neq <- "Check the input variables. Did you specify CO2$Type and CO2$uptake?"

state31 <- ex() %>% check_function("as.numeric")
state31 %>%
  check_arg("x") %>%
  check_equal(incorrect_msg = ex31neq)

state32 <- ex() %>% check_function("cor.test")
state32 %>%
  check_arg("x") %>%
  check_equal(incorrect_msg = ex32neq)
state32 %>%
  check_arg("method") %>%
  check_equal(incorrect_msg = ex33neq)

ex() %>%
  check_function("cor.test") %>%
  check_result() %>%
  check_equal(incorrect_msg = ex34neq)

success_msg("Perfect! Correlations are one of the most important tools available to researchers.")

```


### Activity: Kendall's tau

Using the [cafe] data frame, determine the correlation between the distance driven (_miles_) and the _meal_ eaten. Because meal is categorical but not ordered, use Kendall's _tau_ as the correlation method. Record the correlation in the deliverable document for this lab.

```{r 080.24, ex="act080.03", type="pre-exercise-code"}

cafe <- read.csv('https://labs.basv316.com/cafe.csv')

```

```{r 080.25, ex="act080.03", type="sample-code"}
# Using the cafe data frame, determine the correlation between miles and meal. 



```

-----

## Selecting a Correlation Test

Pearson's _r_, Spearman's _rho_, and Kendall's _tau_ all calculate correlation and it is reasonable to wonder which method should be used in any given situation. Here is a quick chart to help.

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
  Correlation = c("Pearson's r",
                "Spearman's rho",
                "Kendall's tau"
                  ),
  Data = c(
    "both data items being correlated are continuous.",
    "at least one variable is ordered and the sample size is large.",
    "at least one variable is categorical (but not necessarily ordered) and the sample size is small."
  )
)

kable(cor_tbl, 
      "html",
      caption = "Correlation by Data Type") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center"
                ) %>%
  column_spec(1, width = "10em")

```

Imagine a survey where college students were asked to check a box for their class (freshman-sophomore-etc.) and enter their age in years. Spearman's _rho_ would be used to correlate these two data items since the class is ordered ("senior" comes after "junior") and age is continuous.

-----

## Significance

Most people use the word _significant_ to mean _important_ but researchers and statisticians have a much different meaning for the word significant and it is vital to keep that difference in mind.

In statistics and research, significance means that the experimental results were such that they would not likely have been produced by mere chance. For example, if a coin is flipped 100 times, heads should come up 50 times. Of course, by pure chance, it would be possible for heads to come up 55 or even 60 times. However, if heads came up 100 times, researchers would suspect that something unusual was happening (and they would be right!). To a researcher, the central question of significance is "How many times can heads come up and still be considered just pure chance?"

In general, researchers use one of three significance levels: 1%, 5%, or 10%. A researcher conducting The Great Coin-Tossing Experiment may start by simply stating "This result will be significant at the 5% level." That would mean that if the coin were tossed 100 times, then anything between 47.5-52.5 (a 5% spread) "heads" tosses would be considered merely chance. However, 47 or 53 "heads" would be outside that 5% spread and would be significant.

It must seem somewhat subjective for a researcher to simply select the desired significance level, but an overwhelming number researchers in business and the social and behavioral sciences (like education, sociology, and psychology) tend to choose a significance level of 5%. There is no real reason for choosing a 5% level other than it is just the way things have traditionally been done for many years. Therefore, if a researcher selected something other than 5%, peer researchers would want some explanation concerning the "weird" significance level.

The calculated significance is typically reported as a _p-value_ (for "probability value"). The following table contains the correlation and p-value for several pairs of variables from the [mtcars] data frame. 

```{r tut=FALSE, echo=FALSE, message=FALSE, warnings=FALSE}
cor_tbl <- data.frame(
Variables = c(
    "wt—qsec",
    "am—hp",
    "hp—drat",
    "cyl—vs",
    "wt—disp"
  ),
  Correlation = c(
    "-0.1747",
    "-0.2432",
    "-0.4488",
    "-0.8108",
    "+0.8880"
  ),
  Pvalue = c(
    "0.3389",
    "0.1798",
    "0.009989",
    "1.843 x 10^-08^",
    "1.222 x 10^-11^"
  )
)

kable(cor_tbl, 
      "html",
      caption = "P-Values for Selected Correlations") %>%
  kable_styling(bootstrap_options = c("striped","hover","responsive"),
                full_width = F,
                position = "center"
                )

```

If a 5% significance level were specified for this data then any p-value smaller than 0.05 is considered significant; that is, the observed relationship is not likely due to chance. Given that, there is no significance in the correlation between _wt—qsec_ and between _am—hp_ since the p-values for those correlations are greater than 0.05. However, the correlations between the other variables are significant since the p-values for those are smaller than 0.05.

### Demonstration: Significance

One of the statistics calculated by `cor.test` is the p-value, so that test can be used to find significance along with a correlation. The following script demonstrates using `cor.test` to find the significance in the correlation between _Wind_ and _Temp_ in the [airquality] data frame.

```{r 080.28}

# Significance
cor.test(airquality$Wind, airquality$Temp,
  method = "pearson")

```

At the end of line two of the results (not counting the title line), the _p-value_ is reported at <span class="grshilite">2.642e-09</span>, which is how _R_ reports 2.642 &times; 10^-09^, and that is far smaller than 0.05. This correlation, then, would be considered statistically significant.

### Guided Practice: Significance

Using the [CO2] data frame, calculate the p-value for the correlation between _conc_ and _uptake_. Since these are both continuous data, _pearson_ should be selected for the method.

```{r 080.29, ex="sc080.04", type="pre-exercise-code"}

# No pre-exercise code for this exercise

```

```{r 080.30, ex="sc080.04", type="sample-code"}
# Calculate the p-value for conc and uptake in the CO2 data frame.


```

```{r 080.31, ex="sc080.04", type="solution"}

cor.test(CO2$conc, CO2$uptake,
  method = "pearson")

```

```{r 080.32, ex="sc080.04", type="sct"}

ex41neq <- "You should specify CO2$conc and CO2$uptake as the variables. Note: CO2 are capital letters. Be sure the method specified is pearson."

ex() %>%
  check_function("cor.test") %>%
  check_result() %>%
  check_equal(incorrect_msg = ex41neq)

success_msg("Good job! With a p-value of less than 0.05, this correlation is significant.")

```


### Activity: Significance

Using the [cafe] data frame, calculate the p-value for the correlation between _age_ and _bill_. Since these are both continuous data, _pearson_ should be selected for the method. Record the p-value in the deliverable document for this lab.

```{r 080.33, ex="act080.04", type="pre-exercise-code"}

cafe <- read.csv('https://labs.basv316.com/cafe.csv')

```

```{r 080.34, ex="act080.04", type="sample-code"}
# Using the cafe data frame, determine the p-value for age and bill.



```

-----

## Prediction by Regression

A regression line can be drawn on a scatter plot to graphically show the relationship between two variables (this is sometimes called a "trend line" and a "line of best fit"). Moreover, if the data points in the scatter plot are all close to the regression line, then it indicates a strong correlation.

As an example, in the [mtcars] dataset the calculated Pearson's _r_ for Quarter-mile Time and Horsepower is -0.708. These two variables have a strong negative correlation and are plotted below.

```{r 080.35, fig.width=4,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE}

plot(mtcars$hp, mtcars$qsec, 
     type="p", 
     main="Quarter-Mile Time \nby Horsepower", 
     xlab="Horsepower", 
     ylab="Qtr-Mile Time"
     )
# lm(y ~ x, data=d)
lmod <- lm(qsec ~ hp, data = mtcars)
abline(lmod,
       col = "red",
       lwd = 2
       )

```

While regression analysis can have many uses, one of the simplest and most common is to make y-axis predictions for new x-axis values. Using _r's_ linear model function (`lm`), the line drawn in the above figure was determined to have a slope of -0.01846 and a y-intercept of 20.55635. Using this information and the slope intercept equation, $y = mx + b$, the quarter-mile time (y-axis) for a new horsepower (x-axis) can be predicted. For example, the following predicts the quarter-mile time for a horsepower of 200.

$$
\begin{aligned}
y &= mx + b \\
y &= (-0.01846 * 200) + 20.55635 \\
y &= -3.692 + 20.55635 \\
y &= 16.86435
\end{aligned}
$$

In the equation, _m_ is the slope of the regression line and _b_ is the y-intercept. By plugging in the new value for _x_, a simple calculation will determine the predicted value for _y_, which is 16.86 (rounded).

Regression analysis becomes less certain if the supplied _x_ value is at the edge or outside the main body of the scatter plot. For example, using the above figure, it is mathematically possible to predict the quarter-mile time (y-axis) for a horsepower of 50.

$$
\begin{aligned}
y &= mx + b \\
y &= (-0.01846 * 50) + 20.55635 \\
y &= -0.923 + 20.55635 \\
y &= 19.63
\end{aligned}
$$

However, since the selected _x_ value is at the very edge of the main body of the scatter plot then the calculated _y_ value is suspect and should not be reported.

### Demonstration: Prediction by Regression

The following _r_ script uses the `lm` function to find the values of $m$ and $b$ for a correlation between two variables. Then, it calculates a predicted value for _y_ when given an _x_ value. 

* Line 3: The slope-intercept equation is noted in a comment on line 3.
* Line 5: This line executes the `lm` function to generate a "linear model" (the line of best fit) for _horsepower_ (x-axis) and _quarter-mile time_ (y-axis) in the [mtcars] data frame. The result of this function is stored in a variable named _lmod_. It is important to ensure that the variables are entered in the correct order or the linear model will be backwards. The dependent variable (the y-axis) is listed first and then the independent variable (the x-axis). Thus, it is expected that the quarter-mile time is dependent on the horsepower of the car, not the other way around.
* Lines 6-7: Each of these lines store one number in a variable for use in Line 13. There is an odd format used to extract each of these numbers. `lmod` contains a lot of information, including dozens of numbers, and Line 6 (the slope) accesses the `coef` section of `lmod` and extracts just the slope and then stores it in a variable named _b_. Line 7 accesses the `coef` section of `lmod` and extracts just the intercept and stores it in a variable named _m_.
* Line 10: The new value of horsepower is stored in a variable named _x_. In this case, the quarter-mile time for a car with 250 horsepower is being predicted
* Line 13: This is the slope-intercept equation written in _R_ format and it is executed with the variables stored in Lines 6, 7, and 10.

```{r 080.36}
# Demonstration: Prediction by Regression

# Slope-intercept equation: y = mx + b
# First, determine m and b
lmod <- lm(mtcars$qsec ~ mtcars$hp)
b <- coef(lmod)[[1]] # slope
m <- coef(lmod)[[2]] # intercept

# Now, specify the value of x
x <- 250.0

# Calculate the predicted y value
(m * x) + b

```

### Guided Practice: Prediction by Regression

```{r 080.37, fig.width=4,fig.height=4,fig.align="center", echo=FALSE, message=FALSE, warnings=FALSE, include=FALSE}

# This is not included for the students, it's just to help me a bit.

plot(airquality$Temp, airquality$Wind, 
     type="p", 
     main="Wind by Temperature", 
     xlab="Temperature", 
     ylab="Wind"
     )
# lm(y ~ x, data=d)
lmod <- lm(Wind ~ Temp, data = airquality)
abline(lmod,
       col = "blue",
       lwd = 2
       )

```


Using the [airquality] data frame, predict the _Wind_ (y-axis) when the _Temp_ (x-axis) is 80 degrees.

```{r 080.38, ex="sc080.05", type="pre-exercise-code"}

# No pre-exercise code for this exercise

```

```{r 080.39, ex="sc080.05", type="sample-code"}
# Predict the wind for a temp of 80 degrees in the airquality data frame.


```

```{r 080.40, ex="sc080.05", type="solution"}

lmod <- lm(airquality$Wind ~ airquality$Temp)
b <- coef(lmod)[[1]] # slope
m <- coef(lmod)[[2]] # intercept
x <- 80.0
(m * x) + b

```

```{r 080.41, ex="sc080.05", type="sct"}

# These are numbered by the exercise number and a sequence within the exercise.
# The sequence number is not necessary if there is only one check.
ex51neq <- "You should have a linear model of airquality$Wind ~ airquality$Temp"
ex52neq <- "Be sure to load 80 into x."

ex() %>%
  check_object("lmod") %>%
  check_equal(incorrect_msg = ex51neq)

ex() %>%
  check_object("x") %>%
  check_equal(incorrect_msg = ex52neq)

success_msg("Perfect! So the predicted wind on an 80 degree day is 9.596533.")

```

### Activity: Prediction by Regression

Using the [cafe] data frame, predict the tip from a customer who is 48 years old. Round the tip to the nearest penny. Record the prediction in the deliverable document for this lab.

```{r 080.42, ex="act080.05", type="pre-exercise-code"}

cafe <- read.csv('https://labs.basv316.com/cafe.csv')

```

```{r 080.43, ex="act080.05", type="sample-code"}
# Using the cafe dataset, predict the tip from a 48 year-old customer.



```

-----

## Deliverable

Complete the activities in this lab and consolidate the responses into a single document. Name the document with your name and "Lab 7," like "George Self Lab 7" and submit that document for grade.
